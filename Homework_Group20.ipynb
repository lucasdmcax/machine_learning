{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "891cffd7",
   "metadata": {},
   "source": [
    "# Machine Learning Project\n",
    "\n",
    "## Table of Contents\n",
    "- [Import Data](#import-data)\n",
    "  - [Import Data Summary](#import-data-summary)\n",
    "- [Data Exploration](#data-exploration)\n",
    "  - [Boolean Features](#boolean-features)\n",
    "    - [Boolean Features Analysis](#boolean-features-analysis)\n",
    "  - [Categorical Features](#categorical-features)\n",
    "    - [Check Categorical Features Consistency](#check-categorical-features-consistency)\n",
    "    - [Categorical Features Summary](#categorical-features-summary)\n",
    "  - [Numerical Features](#numerical-features)\n",
    "    - [Numerical Plots](#plots)\n",
    "    - [Analysis of Numerical Distributions](#analysis-of-numerical-distributions)\n",
    "- [Pre-processing](#pre-processing)\n",
    "  - [Functions](#functions)\n",
    "  - [Data Preparation](#data-preparation)\n",
    "    - [Correlation Analysis](#correlation-analysis)\n",
    "- [Model Training](#model-training)\n",
    "  - [Quick Baseline Model](#quick-baseline-model)\n",
    "  - [Experiment Algorithms](#experiment-algorithms)\n",
    "- [Predictions](#predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47693163",
   "metadata": {},
   "source": [
    "<a id=\"import-data\"></a>\n",
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9c6c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error, r2_score\n",
    "from collections import Counter\n",
    "from scipy.stats import loguniform, randint\n",
    "import nltk\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6468f612",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv').set_index('carID')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc133ea3-1850-4386-ab5e-6db9950c48f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a5a91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_duplicated_ids = df.index.duplicated().sum()\n",
    "print(f'Number of duplicated carIDs: {num_duplicated_ids}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c607395",
   "metadata": {},
   "source": [
    "<a id=\"import-data-summary\"></a>\n",
    "#### Import Data Summary\n",
    "- Dataset loaded successfully with `carID` as the index\n",
    "- There are no duplicate entries in carID\n",
    "- The dataset contains information about cars including both numerical features (price, mileage, tax, etc.) and categorical features (brand, model, transmission, etc.)\n",
    "- Initial inspection shows multiple features that will require preprocessing:\n",
    "  - Numerical features that need cleaning (negative values, outliers)\n",
    "  - Categorical features that need standardization\n",
    "  - Presence of missing values in several columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3db49c",
   "metadata": {},
   "source": [
    "<a id=\"data-exploration\"></a>\n",
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb63f270",
   "metadata": {},
   "source": [
    "<a id=\"boolean-features\"></a>\n",
    "### Boolean Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e5927d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hasDamage'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb51349",
   "metadata": {},
   "source": [
    "<a id=\"boolean-features-analysis\"></a>\n",
    "#### Boolean Features Analysis\n",
    "\n",
    "Key observations about `hasDamage` feature:\n",
    "- Only contains binary values (0) and NaN\n",
    "- No instances of value 1 found, suggesting potential data collection issues\n",
    "- May indicate:\n",
    "  - Cars with damage not being listed\n",
    "  - System default setting of 0 for non-damaged cars\n",
    "  - Incomplete damage assessment process\n",
    "- Requires special handling in preprocessing:\n",
    "  - Consider treating NaN as a separate category\n",
    "  - Validate if 0 truly represents \"no damage\"\n",
    "  - May need to be treated as a categorical rather than boolean feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0273ca",
   "metadata": {},
   "source": [
    "<a id=\"categorical-features\"></a>\n",
    "### Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4432c021",
   "metadata": {},
   "source": [
    "<a id=\"check-categorical-features-consistency\"></a>\n",
    "#### Check Categorical Features Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095f1162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of categorical features\n",
    "cat_cols = ['Brand', 'model', 'fuelType', 'transmission']\n",
    "\n",
    "# Identify outlier examples in categorical features\n",
    "cat_outliers_examples = {col: df[col].value_counts().tail(10).index for col in cat_cols}\n",
    "\n",
    "# Display the outlier examples\n",
    "pd.DataFrame(cat_outliers_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa457064",
   "metadata": {},
   "source": [
    "<a id=\"categorical-features-summary\"></a>\n",
    "#### Categorical Features Summary\n",
    "- Initial analysis reveals significant data quality issues across all categorical columns\n",
    "- No standardization in categorical features, with multiple variations of the same values (different spellings, capitalizations)\n",
    "- Solution: We will implement string distance-based standardization using the `thefuzz` library to clean and standardize these features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ab1e0a",
   "metadata": {},
   "source": [
    "<a id=\"numerical-features\"></a>\n",
    "### Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6a26e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict of numerical features\n",
    "num_cols = {\n",
    "    'price': 'continuous',\n",
    "    'mileage': 'continuous',\n",
    "    'tax': 'continuous',\n",
    "    'mpg': 'continuous',\n",
    "    'paintQuality%': 'continuous',\n",
    "    'engineSize': 'continuous',\n",
    "    'year': 'discrete',\n",
    "    'previousOwners': 'discrete'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15d6909",
   "metadata": {},
   "source": [
    "<a id=\"plots\"></a>\n",
    "#### Numerical Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d22957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot figures for numerical features and the target variable (price)\n",
    "plt.figure(figsize=(16, 10))\n",
    "for i, (col, var_type) in enumerate(num_cols.items(), 1):\n",
    "    plt.subplot(4, 2, i)\n",
    "\n",
    "    # Plot based on variable type\n",
    "    if var_type == 'continuous':\n",
    "        sns.histplot(data=df, x=col, kde=True, color=\"lightcoral\", bins=30)\n",
    "        plt.title(f\"Distribution of {col}\", fontsize=11)\n",
    "    elif var_type == 'discrete':\n",
    "        sns.countplot(data=df, x=col, color=\"lightcoral\")\n",
    "        plt.title(f\"Distribution of {col}\", fontsize=11)\n",
    "        plt.xticks(rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f095a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots for continuous numerical features and the target variable (price)\n",
    "continuous_cols = [col for col, var_type in num_cols.items() if var_type == 'continuous']\n",
    "plt.figure(figsize=(16, 10))\n",
    "for i, col in enumerate(continuous_cols, 1):\n",
    "    plt.subplot(3, 2, i)\n",
    "    sns.boxplot(data=df, x=col, color=\"lightblue\")\n",
    "    plt.title(f\"Distribution of {col}\", fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0576a8e8",
   "metadata": {},
   "source": [
    "<a id=\"analysis-of-numerical-distributions\"></a>\n",
    "#### Analysis of Numerical Distributions\n",
    "\n",
    "Key observations from the plots:\n",
    "- **Target Variable (Price)**:\n",
    "  - Highly right-skewed distribution\n",
    "  - Contains significant number of outliers in the upper range\n",
    "  - Most cars are concentrated in the lower price range\n",
    "\n",
    "- **Mileage**:\n",
    "  - Right-skewed distribution\n",
    "  - Large range from nearly new cars to high-mileage vehicles\n",
    "  - Some outliers in upper range suggesting possible data entry errors\n",
    "\n",
    "- **Tax**:\n",
    "  - Multiple peaks suggesting different tax bands\n",
    "  - Contains negative values which require investigation (possible tax benefits/rebates)\n",
    "  - Large number of outliers on both ends of the distribution\n",
    "\n",
    "- **MPG (Miles Per Gallon)**:\n",
    "  - Approximately normal distribution with slight right skew\n",
    "  - Some unrealistic extreme values that need cleaning\n",
    "  - Reasonable median around typical car efficiency ranges\n",
    "\n",
    "- **Paint Quality %**:\n",
    "  - Contains values above 100% which are logically impossible\n",
    "  - Left-skewed distribution suggesting optimistic ratings\n",
    "  - Requires standardization to 0-100 range\n",
    "\n",
    "- **Engine Size**:\n",
    "  - There are engine size with zero values which are not realistic (might indicate electric vehicles)\n",
    "  - Some unusual patterns that need investigation\n",
    "  - Contains outliers that may represent specialty vehicles\n",
    "\n",
    "- **Year**:\n",
    "  - Should be discrete but contains decimal values\n",
    "\n",
    "- **Previous Owners**:\n",
    "  - Should be integer but contains float values\n",
    "  - Right-skewed distribution as expected\n",
    "  - Maximum values need validation (unusually high number of previous owners)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f2c3cb",
   "metadata": {},
   "source": [
    "<a id=\"pre-processing\"></a>\n",
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0225083",
   "metadata": {},
   "source": [
    "<a id=\"functions\"></a>\n",
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6c0933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  general_cleaning(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Perform general data cleaning on the DataFrame.\n",
    "    \n",
    "    This function handles logical inconsistencies and data quality issues that\n",
    "    don't require statistical calculations (mean, median, etc.) to prevent data\n",
    "    leakage between training and validation sets.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame containing car data with columns:\n",
    "            Brand, model, year, transmission, fuelType, mileage, tax, mpg, \n",
    "            engineSize, paintQuality%, previousOwners, hasDamage.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: The cleaned DataFrame with logical inconsistencies resolved.\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # Set negative values to NaN for features that shouldn't be negative\n",
    "    for col in ['previousOwners', 'mileage', 'tax', 'mpg', 'engineSize']:\n",
    "        df.loc[df[col] < 0, col] = np.nan\n",
    "\n",
    "    for col in ['Brand', 'model', 'transmission', 'fuelType']:\n",
    "        df[col] = df[col].str.lower()\n",
    "        df[col] = df[col].replace('', np.nan)\n",
    "\n",
    "    # Remove decimal part from 'year'\n",
    "    df['year'] = np.floor(df['year']).astype('Int64')\n",
    "\n",
    "    # Remove decimal part from 'previousOwners'\n",
    "    df['previousOwners'] = np.floor(df['previousOwners']).astype('Int64')\n",
    "\n",
    "    # Ensure 'paintQuality%' is within 0-100\n",
    "    df.loc[(df['paintQuality%'] < 0) | (df['paintQuality%'] > 100), 'paintQuality%'] = np.nan\n",
    "\n",
    "    # Fill missing 'hasDamage' with 1\n",
    "    df['hasDamage'] = df['hasDamage'].fillna(1).astype('Int64')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48fcfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_categorical_col(series: pd.Series, \n",
    "                                standardised_cats: list[str], \n",
    "                                distance_threshold: int = 2) -> pd.Series:\n",
    "    \"\"\"Standardizes a categorical column using edit distance with a threshold.\n",
    "\n",
    "    1. Maps values to a standard category if they are a likely typo\n",
    "       (i.e., within the edit distance_threshold).\n",
    "    2. Keeps values that are already in the standard list.\n",
    "    3. Groups all other values that don't match into an 'other' bin.\n",
    "    \n",
    "    Args:\n",
    "        series (pd.Series): The categorical column to standardize.\n",
    "        standardised_cats (list[str]): The list of \"good\" categories to match against.\n",
    "        distance_threshold (int): The max edit distance to consider something a typo.\n",
    "                                  A value of 1 or 2 is recommended.\n",
    "                                  \n",
    "    Returns:\n",
    "        pd.Series: The standardized categorical column.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Get all unique non-null values from the series\n",
    "    unique_values = series.dropna().unique()\n",
    "    \n",
    "    # 2. Build the mapping dictionary\n",
    "    mapping = {}\n",
    "    \n",
    "    for x in unique_values:\n",
    "        x_str = str(x)\n",
    "        \n",
    "        # Check if it's already a perfect match\n",
    "        if x_str in standardised_cats:\n",
    "            mapping[x] = x_str\n",
    "            continue\n",
    "\n",
    "        # Find the closest match and its distance\n",
    "        distances = [nltk.edit_distance(x_str, cat) for cat in standardised_cats]\n",
    "        min_distance = np.min(distances)\n",
    "        \n",
    "        if min_distance <= distance_threshold:\n",
    "            closest_cat = standardised_cats[np.argmin(distances)]\n",
    "            mapping[x] = closest_cat\n",
    "        else:\n",
    "            mapping[x] = 'other'\n",
    "            \n",
    "    return series.map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9309596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_high_freq(series: pd.Series, percent_threshold: float = 0.02) -> list[str]:\n",
    "    \"\"\"Get categories that appear more than a dynamic percentage threshold.\n",
    "    \n",
    "    Args:\n",
    "        series (pd.Series): The categorical series to analyze.\n",
    "        percent_threshold (float): The minimum percentage of total rows a category\n",
    "                                   must have to be included (e.g., 0.01 for 1%).\n",
    "                                   \n",
    "    Returns:\n",
    "        list[str]: List of categories with frequency above the dynamic threshold.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the dynamic count threshold based on the percentage\n",
    "    dynamic_count_threshold = len(series) * percent_threshold\n",
    "    \n",
    "    value_counts = series.value_counts()\n",
    "    \n",
    "    # Use the *same logic* as before, but with the new dynamic threshold\n",
    "    high_freq_cats = value_counts[value_counts > dynamic_count_threshold].index.tolist()\n",
    "    \n",
    "    return high_freq_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1795c746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_upper_bound(series: pd.Series) -> float:\n",
    "    \"\"\"Calculates the upper outlier bound for a pandas Series.\"\"\"\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    return Q3 + (1.5 * IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3254edf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_outliers(series: pd.Series, \n",
    "                   upper_bound: float,\n",
    "                   lower_bound: float = 0.0, \n",
    "                   return_missing: bool = True) -> pd.Series:\n",
    "    \"\"\"Clean outliers in the Series based on specified bounds.\n",
    "\n",
    "    This function clips values outside the specified bounds or sets them to NaN.\n",
    "    \n",
    "    Args:\n",
    "        series (pd.Series): The input Series containing numerical data.\n",
    "        lower_bound (float): The lower bound for valid values.\n",
    "        upper_bound (float): The upper bound for valid values.\n",
    "        return_missing (bool): If True, set out-of-bound values to NaN.\n",
    "                              If False, clip values to the bounds.\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series: The cleaned Series with outliers handled.\n",
    "    \"\"\"\n",
    "    cleaned = series.copy()\n",
    "    \n",
    "    if return_missing:\n",
    "        # Set out-of-bound values to NaN\n",
    "        cleaned[(cleaned < lower_bound) | (cleaned > upper_bound)] = np.nan\n",
    "    else:\n",
    "        # Clip values to the specified bounds\n",
    "        cleaned = cleaned.clip(lower=lower_bound, upper=upper_bound)\n",
    "    \n",
    "    return cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372364ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X, cat_cols, num_cols, artifacts=None, fit=True):\n",
    "    \"\"\"\n",
    "    Preprocess data using consistent transformations.\n",
    "    \n",
    "    Args:\n",
    "        X (pd.DataFrame): Features to preprocess\n",
    "        cat_cols (list): Categorical column names\n",
    "        num_cols (dict): Numerical column names with types\n",
    "        artifacts (dict): Preprocessing artifacts (if fit=False)\n",
    "        fit (bool): If True, fit transformers; if False, use provided artifacts\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (X_processed, artifacts) if fit=True, else X_processed\n",
    "    \"\"\"\n",
    "    X = X.copy()\n",
    "    continuous_cols = [col for col, var_type in num_cols.items() if var_type == 'continuous']\n",
    "    \n",
    "    if fit:\n",
    "        # Fit preprocessing on training data\n",
    "        high_freq_cats = {col: get_categories_high_freq(X[col]) for col in cat_cols}\n",
    "        mileage_upper = X['mileage'].quantile(0.95)\n",
    "        outlier_bounds = {col: calculate_upper_bound(X[col]) for col in continuous_cols}\n",
    "        medians = {col: X[col].median() for col in num_cols}\n",
    "        \n",
    "        artifacts = {\n",
    "            'high_freq_cats': high_freq_cats,\n",
    "            'mileage_upper': mileage_upper,\n",
    "            'outlier_bounds': outlier_bounds,\n",
    "            'medians': medians,\n",
    "            'cat_cols': cat_cols,\n",
    "            'num_cols': num_cols\n",
    "        }\n",
    "    else:\n",
    "        high_freq_cats = artifacts['high_freq_cats']\n",
    "        mileage_upper = artifacts['mileage_upper']\n",
    "        outlier_bounds = artifacts['outlier_bounds']\n",
    "        medians = artifacts['medians']\n",
    "    \n",
    "    # 1. Categorical preprocessing\n",
    "    for col in cat_cols:\n",
    "        X[col] = standardize_categorical_col(X[col], high_freq_cats[col])\n",
    "        X[col] = X[col].fillna('other')\n",
    "    \n",
    "    # 2. Numerical outliers\n",
    "    X['mileage'] = clean_outliers(X['mileage'], mileage_upper, 0, return_missing=False)\n",
    "    \n",
    "    for col in continuous_cols:\n",
    "        if col != 'mileage':\n",
    "            X[col] = clean_outliers(X[col], outlier_bounds[col])\n",
    "    \n",
    "    # 3. Fill missing values\n",
    "    for col in num_cols:\n",
    "        X[col] = X[col].fillna(medians[col])\n",
    "    \n",
    "    # 4. One-hot encoding\n",
    "    if fit:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "        ohe_data = pd.DataFrame(\n",
    "            ohe.fit_transform(X[cat_cols]),\n",
    "            columns=ohe.get_feature_names_out(cat_cols),\n",
    "            index=X.index\n",
    "        )\n",
    "        artifacts['encoder'] = ohe\n",
    "    else:\n",
    "        ohe_data = pd.DataFrame(\n",
    "            artifacts['encoder'].transform(X[cat_cols]),\n",
    "            columns=artifacts['encoder'].get_feature_names_out(cat_cols),\n",
    "            index=X.index\n",
    "        )\n",
    "    \n",
    "    X = pd.concat([X.drop(columns=cat_cols), ohe_data], axis=1)\n",
    "    \n",
    "    # 5. Normalize numerical features\n",
    "    numerical_cols = list(num_cols.keys())\n",
    "    if fit:\n",
    "        scaler = StandardScaler()\n",
    "        X[numerical_cols] = scaler.fit_transform(X[numerical_cols])\n",
    "        artifacts['scaler'] = scaler\n",
    "    else:\n",
    "        X[numerical_cols] = artifacts['scaler'].transform(X[numerical_cols])\n",
    "    \n",
    "    return (X, artifacts) if fit else X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973d43c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_hyperparameters(param_distributions, n_iter, seed):\n",
    "    \"\"\"\n",
    "    Sample hyperparameters from distributions.\n",
    "    \n",
    "    Args:\n",
    "        param_distributions (dict): Dictionary with parameter names as keys and\n",
    "                                   distributions/lists as values\n",
    "        n_iter (int): Number of parameter combinations to sample\n",
    "        seed (int): Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        list[dict]: List of parameter dictionaries\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    param_list = []\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        params = {}\n",
    "        for param_name, param_values in param_distributions.items():\n",
    "            # Check if it's a scipy distribution (has .rvs method)\n",
    "            if hasattr(param_values, 'rvs'):\n",
    "                params[param_name] = param_values.rvs(random_state=seed + i)\n",
    "            # Check if it's a list (discrete choices)\n",
    "            elif isinstance(param_values, list):\n",
    "                params[param_name] = np.random.choice(param_values)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown parameter type for {param_name}\")\n",
    "        \n",
    "        param_list.append(params)\n",
    "    \n",
    "    return param_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e32457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_with_tuning(X_raw, y_raw, cat_cols_list, num_cols_dict, model_config, k=3, seed=42):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross-validation with manual hyperparameter search.\n",
    "    Preprocessing is done within each fold to prevent data leakage.\n",
    "    \n",
    "    Args:\n",
    "        X_raw (pd.DataFrame): Raw training features (not preprocessed)\n",
    "        y_raw (pd.Series): Raw training target (not log-transformed)\n",
    "        cat_cols_list (list): List of categorical column names\n",
    "        num_cols_dict (dict): Dictionary of numerical columns with types\n",
    "        model_config (dict): Configuration dictionary with keys:\n",
    "            - 'model_class': sklearn model class (e.g., Ridge, Lasso, RandomForestRegressor)\n",
    "            - 'param_distributions': dict of parameter distributions for sampling\n",
    "            - 'n_iter': number of parameter settings to sample (default: 20)\n",
    "        k (int): Number of CV folds (default: 3)\n",
    "        seed (int): Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        dict: Results with best_params, best_estimator, CV scores, and preprocessing artifacts\n",
    "    \"\"\"\n",
    "    # Setup CV\n",
    "    kfold = KFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "    \n",
    "    # Sample hyperparameters once (will be used across all folds)\n",
    "    n_iter = model_config.get('n_iter', 20)\n",
    "    param_combinations = sample_hyperparameters(\n",
    "        model_config['param_distributions'], \n",
    "        n_iter, \n",
    "        seed\n",
    "    )\n",
    "    \n",
    "    # Storage for results\n",
    "    fold_results = []\n",
    "    best_score_overall = float('inf')\n",
    "    best_params_overall = None\n",
    "    best_estimator_overall = None\n",
    "    best_artifacts = None\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Starting {k}-Fold Cross-Validation with Hyperparameter Tuning\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Model: {model_config['model_class'].__name__}\")\n",
    "    print(f\"Parameter space: {model_config['param_distributions']}\")\n",
    "    print(f\"Hyperparameter combinations: {n_iter}\")\n",
    "    print(f\"Total model fits: {k * n_iter}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Perform manual CV with preprocessing in each fold\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(X_raw), 1):\n",
    "        print(f\"Fold {fold_idx}/{k}\")\n",
    "        print(f\"{'-'*40}\")\n",
    "        \n",
    "        # Split data for this fold\n",
    "        X_train_fold = X_raw.iloc[train_idx].copy()\n",
    "        X_val_fold = X_raw.iloc[val_idx].copy()\n",
    "        y_train_fold = y_raw.iloc[train_idx].copy()\n",
    "        y_val_fold = y_raw.iloc[val_idx].copy()\n",
    "        \n",
    "        # Preprocess data for this fold\n",
    "        X_train_processed, fold_artifacts = preprocess_data(\n",
    "            X_train_fold, cat_cols_list, num_cols_dict, fit=True\n",
    "        )\n",
    "        X_val_processed = preprocess_data(\n",
    "            X_val_fold, cat_cols_list, num_cols_dict, \n",
    "            artifacts=fold_artifacts, fit=False\n",
    "        )\n",
    "        \n",
    "        # Log-transform target\n",
    "        y_train_log = np.log1p(y_train_fold)\n",
    "        \n",
    "        # Hyperparameter tuning: try each parameter combination\n",
    "        best_fold_score = float('inf')\n",
    "        best_fold_params = None\n",
    "        best_fold_model = None\n",
    "        \n",
    "        for param_idx, params in enumerate(param_combinations, 1):\n",
    "            # Train model with these parameters\n",
    "            model = model_config['model_class'](**params)\n",
    "            model.fit(X_train_processed, y_train_log)\n",
    "            \n",
    "            # Predict on validation fold\n",
    "            y_val_pred_log = model.predict(X_val_processed)\n",
    "            y_val_pred = np.expm1(y_val_pred_log)\n",
    "            \n",
    "            # Calculate MAE\n",
    "            fold_mae = mean_absolute_error(y_val_fold, y_val_pred)\n",
    "            \n",
    "            # Track best for this fold\n",
    "            if fold_mae < best_fold_score:\n",
    "                best_fold_score = fold_mae\n",
    "                best_fold_params = params\n",
    "                best_fold_model = model\n",
    "            \n",
    "            # Progress indicator every 5 iterations\n",
    "            if param_idx % 5 == 0 or param_idx == n_iter:\n",
    "                print(f\"  Evaluated {param_idx}/{n_iter} parameter combinations...\")\n",
    "        \n",
    "        # Calculate train performance for best model\n",
    "        y_train_pred_log = best_fold_model.predict(X_train_processed)\n",
    "        y_train_pred = np.expm1(y_train_pred_log)\n",
    "        train_mae = mean_absolute_error(y_train_fold, y_train_pred)\n",
    "        \n",
    "        print(f\"  Best params: {best_fold_params}\")\n",
    "        print(f\"  Train MAE: £{train_mae:.2f}\")\n",
    "        print(f\"  Validation MAE: £{best_fold_score:.2f}\\n\")\n",
    "        \n",
    "        # Store fold results\n",
    "        fold_results.append({\n",
    "            'fold': fold_idx,\n",
    "            'best_params': best_fold_params,\n",
    "            'train_mae': train_mae,\n",
    "            'val_mae': best_fold_score,\n",
    "            'best_model': best_fold_model,\n",
    "            'artifacts': fold_artifacts\n",
    "        })\n",
    "        \n",
    "        # Track overall best across all folds\n",
    "        if best_fold_score < best_score_overall:\n",
    "            best_score_overall = best_fold_score\n",
    "            best_params_overall = best_fold_params\n",
    "            best_estimator_overall = best_fold_model\n",
    "            best_artifacts = fold_artifacts\n",
    "    \n",
    "    # Calculate mean and std of CV scores\n",
    "    cv_scores = [r['val_mae'] for r in fold_results]\n",
    "    mean_cv_score = np.mean(cv_scores)\n",
    "    std_cv_score = np.std(cv_scores)\n",
    "    \n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Cross-Validation Results:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Mean CV MAE: £{mean_cv_score:.2f} ± £{std_cv_score:.2f}\")\n",
    "    print(f\"Best Fold MAE: £{best_score_overall:.2f}\")\n",
    "    print(f\"Best Parameters: {best_params_overall}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Train final model on all data using best parameters\n",
    "    print(\"Training final model on all data with best parameters...\")\n",
    "    X_all_processed, final_artifacts = preprocess_data(\n",
    "        X_raw, cat_cols_list, num_cols_dict, fit=True\n",
    "    )\n",
    "    y_all_log = np.log1p(y_raw)\n",
    "    \n",
    "    final_model = model_config['model_class'](**best_params_overall)\n",
    "    final_model.fit(X_all_processed, y_all_log)\n",
    "    \n",
    "    print(\"Final model trained successfully!\\n\")\n",
    "    \n",
    "    return {\n",
    "        'best_params': best_params_overall,\n",
    "        'best_estimator': final_model,\n",
    "        'mean_cv_score': mean_cv_score,\n",
    "        'std_cv_score': std_cv_score,\n",
    "        'best_fold_score': best_score_overall,\n",
    "        'fold_results': fold_results,\n",
    "        'preprocessing_artifacts': final_artifacts\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fda072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_test_data(test_df, artifacts):\n",
    "    \"\"\"\n",
    "    Preprocess test data using artifacts from training.\n",
    "    \n",
    "    Args:\n",
    "        test_df (pd.DataFrame): Raw test dataframe\n",
    "        artifacts (dict): Preprocessing artifacts from cross_validate_with_tuning\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Preprocessed test data ready for prediction\n",
    "    \"\"\"\n",
    "    # General cleaning\n",
    "    test_cleaned = general_cleaning(test_df)\n",
    "    \n",
    "    # Apply preprocessing using artifacts\n",
    "    test_processed = preprocess_data(\n",
    "        test_cleaned, \n",
    "        artifacts['cat_cols'], \n",
    "        artifacts['num_cols'], \n",
    "        artifacts=artifacts, \n",
    "        fit=False\n",
    "    )\n",
    "    \n",
    "    return test_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446f22c3",
   "metadata": {},
   "source": [
    "## Summary of Preprocessing Pipeline\n",
    "\n",
    "The preprocessing is now properly separated:\n",
    "\n",
    "1. **`preprocess_data()`** - Preprocesses a single dataset\n",
    "   - Handles categorical features (standardization, encoding)\n",
    "   - Handles numerical outliers using IQR method\n",
    "   - Imputes missing values with medians\n",
    "   - One-hot encodes categorical features\n",
    "   - Normalizes numerical features with StandardScaler\n",
    "   - Can fit transformers (fit=True) or use existing ones (fit=False)\n",
    "\n",
    "2. **`cross_validate_with_tuning()`** - Performs CV with hyperparameter tuning\n",
    "   - Takes **raw data** (after general_cleaning)\n",
    "   - Applies preprocessing **separately for each fold** (prevents data leakage)\n",
    "   - Performs manual hyperparameter search by sampling from parameter distributions\n",
    "   - Evaluates each combination on validation fold and tracks train/validation performance\n",
    "   - Returns best model trained on all data + preprocessing artifacts\n",
    "\n",
    "3. **`preprocess_test_data()`** - Preprocesses test data\n",
    "   - Uses artifacts from CV to ensure consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e972a0",
   "metadata": {},
   "source": [
    "<a id=\"data-preparation\"></a>\n",
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9194c9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare cleaned data for cross-validation\n",
    "df_cleaned = general_cleaning(df)\n",
    "X = df_cleaned.drop(columns=[\"price\"])\n",
    "y = df_cleaned[\"price\"]\n",
    "\n",
    "# Remove 'price' from num_cols since it's the target\n",
    "del num_cols['price']\n",
    "\n",
    "print(f\"Dataset size: {X.shape}\")\n",
    "print(f\"Target range: £{y.min():.2f} - £{y.max():.2f}\")\n",
    "print(f\"\\nReady for cross-validation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cc5ab5",
   "metadata": {},
   "source": [
    "<a id=\"correlation-analysis\"></a>\n",
    "#### Correlation Analysis\n",
    "\n",
    "Before model training, let's examine correlations between numerical features to understand their relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4e09b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for numerical features\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "corr = X[list(num_cols.keys())].corr(method=\"pearson\")\n",
    "sns.heatmap(data=corr, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Matrix of Numerical Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f115e4",
   "metadata": {},
   "source": [
    "<a id=\"model-training\"></a>\n",
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfc3b63",
   "metadata": {},
   "source": [
    "<a id=\"model-selection-with-cv\"></a>\n",
    "### Model Selection with Cross-Validation\n",
    "\n",
    "We'll use cross-validation with hyperparameter tuning to select the best model. Configure your model using a dictionary with the model class, parameter distributions, and number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e1e089",
   "metadata": {},
   "source": [
    "<a id=\"quick-baseline-model\"></a>\n",
    "### Quick Baseline Model\n",
    "\n",
    "Before running extensive CV, let's train a simple baseline model for quick reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a55e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick train/val split for baseline\n",
    "X_train_baseline, X_val_baseline, y_train_baseline, y_val_baseline = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=SEED\n",
    ")\n",
    "\n",
    "# Preprocess baseline data\n",
    "X_train_processed, baseline_artifacts = preprocess_data(X_train_baseline, cat_cols, num_cols, fit=True)\n",
    "X_val_processed = preprocess_data(X_val_baseline, cat_cols, num_cols, artifacts=baseline_artifacts, fit=False)\n",
    "\n",
    "# Train simple Ridge model\n",
    "baseline_model = Ridge(alpha=1.0, fit_intercept=True)\n",
    "baseline_model.fit(X_train_processed, np.log1p(y_train_baseline))\n",
    "\n",
    "# Evaluate\n",
    "y_train_pred = np.expm1(baseline_model.predict(X_train_processed))\n",
    "y_val_pred = np.expm1(baseline_model.predict(X_val_processed))\n",
    "\n",
    "mae_train = mean_absolute_error(y_train_baseline, y_train_pred)\n",
    "mae_val = mean_absolute_error(y_val_baseline, y_val_pred)\n",
    "r2_val = r2_score(y_val_baseline, y_val_pred)\n",
    "\n",
    "print(f\"Baseline Ridge (alpha=1.0):\")\n",
    "print(f\"  Train MAE: £{mae_train:.2f}\")\n",
    "print(f\"  Val MAE:   £{mae_val:.2f}\")\n",
    "print(f\"  Val R²:    {r2_val:.4f}\")\n",
    "print(f\"\\nThis gives us a reference point before hyperparameter tuning with CV.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb601b4",
   "metadata": {},
   "source": [
    "<a id=\"experiment-algorithms\"></a>\n",
    "### Experiment Algorithms\n",
    "\n",
    "Now we'll experiment with different algorithms using cross-validation with hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab6b358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Ridge Regression with hyperparameter tuning\n",
    "ridge_config = {\n",
    "    'model_class': Ridge,\n",
    "    'param_distributions': {\n",
    "        'alpha': loguniform(1e-3, 1e2),\n",
    "        'fit_intercept': [True, False]\n",
    "    },\n",
    "    'n_iter': 20\n",
    "}\n",
    "\n",
    "ridge_results = cross_validate_with_tuning(X, y, cat_cols, num_cols, ridge_config, k=3, seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ebdfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Lasso Regression\n",
    "lasso_config = {\n",
    "    'model_class': Lasso,\n",
    "    'param_distributions': {\n",
    "        'alpha': loguniform(1e-3, 1e2),\n",
    "        'fit_intercept': [True, False]\n",
    "    },\n",
    "    'n_iter': 20\n",
    "}\n",
    "\n",
    "lasso_results = cross_validate_with_tuning(X, y, cat_cols, num_cols, lasso_config, k=3, seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8fb232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Random Forest\n",
    "rf_config = {\n",
    "    'model_class': RandomForestRegressor,\n",
    "    'param_distributions': {\n",
    "        'n_estimators': randint(50, 200),\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': randint(2, 10),\n",
    "        'min_samples_leaf': randint(1, 5)\n",
    "    },\n",
    "    'n_iter': 4\n",
    "}\n",
    "\n",
    "rf_results = cross_validate_with_tuning(X, y, cat_cols, num_cols, rf_config, k=3, seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652a272f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model based on CV results\n",
    "best_result = ridge_results  # Choose: ridge_results, lasso_results, or rf_results\n",
    "best_model = best_result['best_estimator']\n",
    "preprocessing_artifacts = best_result['preprocessing_artifacts']\n",
    "\n",
    "print(f\"\\nSelected Model: {best_model.__class__.__name__}\")\n",
    "print(f\"CV Performance: MAE = £{best_result['mean_cv_score']:.2f} ± £{best_result['std_cv_score']:.2f}\")\n",
    "print(f\"Best Parameters: {best_result['best_params']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb360862",
   "metadata": {},
   "source": [
    "<a id=\"predictions\"></a>\n",
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb33e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess test data\n",
    "test_df = pd.read_csv('data/test.csv').set_index('carID')\n",
    "test_processed = preprocess_test_data(test_df, preprocessing_artifacts)\n",
    "\n",
    "# Make predictions\n",
    "test_predictions = np.expm1(best_model.predict(test_processed))\n",
    "\n",
    "# Save predictions\n",
    "predictions_df = pd.DataFrame({'price': test_predictions}, index=test_df.index)\n",
    "predictions_df.to_csv('data/test_predictions.csv')\n",
    "\n",
    "print(f\"Predictions saved for {len(test_predictions)} test samples\")\n",
    "print(f\"Predicted price range: £{test_predictions.min():.2f} - £{test_predictions.max():.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
