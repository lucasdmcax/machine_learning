{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "891cffd7",
   "metadata": {},
   "source": [
    "# Machine Learning Project\n",
    "\n",
    "## Table of Contents\n",
    "- [Import Data](#import-data)\n",
    "  - [Import Data Summary](#import-data-summary)\n",
    "- [Data Exploration](#data-exploration)\n",
    "  - [Boolean Features](#boolean-features)\n",
    "    - [Boolean Features Analysis](#boolean-features-analysis)\n",
    "  - [Categorical Features](#categorical-features)\n",
    "    - [Check Categorical Features Consistency](#check-categorical-features-consistency)\n",
    "    - [Categorical Features Summary](#categorical-features-summary)\n",
    "  - [Numerical Features](#numerical-features)\n",
    "    - [Numerical Plots](#plots)\n",
    "    - [Analysis of Numerical Distributions](#analysis-of-numerical-distributions)\n",
    "- [Pre-processing](#pre-processing)\n",
    "  - [Functions](#functions)\n",
    "  - [Split Data](#split-data)\n",
    "    - [Target Transformation](#target-transformation)\n",
    "  - [Outliers and Missing Values](#outliers-and-missing-values)\n",
    "    - [Categorical Outliers and Missing Values](#categorical-outliers-and-missing-values)\n",
    "    - [Numerical Outliers](#numerical-outliers)\n",
    "    - [Numerical Missing Values](#numerical-missing-values)\n",
    "- [Feature Engineering](#feature-engineering)\n",
    "  - [Encode Features](#encode-features)\n",
    "  - [Normalize Features](#normalize-features)\n",
    "    - [Correlation Matrix](#correlation-matrix)\n",
    "- [Model Training](#model-training)\n",
    "  - [Baseline Model Training](#baseline-model-training)\n",
    "  - [Experiment with Different Models](#experiment-with-different-models)\n",
    "- [Predictions](#predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47693163",
   "metadata": {},
   "source": [
    "<a id=\"import-data\"></a>\n",
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9c6c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error, r2_score\n",
    "\n",
    "# to calculate distance between strings\n",
    "import nltk\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6468f612",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv').set_index('carID')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc133ea3-1850-4386-ab5e-6db9950c48f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a5a91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_duplicated_ids = df.index.duplicated().sum()\n",
    "print(f'Number of duplicated carIDs: {num_duplicated_ids}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c607395",
   "metadata": {},
   "source": [
    "<a id=\"import-data-summary\"></a>\n",
    "#### Import Data Summary\n",
    "- Dataset loaded successfully with `carID` as the index\n",
    "- There are no duplicate entries in carID\n",
    "- The dataset contains information about cars including both numerical features (price, mileage, tax, etc.) and categorical features (brand, model, transmission, etc.)\n",
    "- Initial inspection shows multiple features that will require preprocessing:\n",
    "  - Numerical features that need cleaning (negative values, outliers)\n",
    "  - Categorical features that need standardization\n",
    "  - Presence of missing values in several columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3db49c",
   "metadata": {},
   "source": [
    "<a id=\"data-exploration\"></a>\n",
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb63f270",
   "metadata": {},
   "source": [
    "<a id=\"boolean-features\"></a>\n",
    "### Boolean Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e5927d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hasDamage'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb51349",
   "metadata": {},
   "source": [
    "<a id=\"boolean-features-analysis\"></a>\n",
    "#### Boolean Features Analysis\n",
    "\n",
    "Key observations about `hasDamage` feature:\n",
    "- Only contains binary values (0) and NaN\n",
    "- No instances of value 1 found, suggesting potential data collection issues\n",
    "- May indicate:\n",
    "  - Cars with damage not being listed\n",
    "  - System default setting of 0 for non-damaged cars\n",
    "  - Incomplete damage assessment process\n",
    "- Requires special handling in preprocessing:\n",
    "  - Consider treating NaN as a separate category\n",
    "  - Validate if 0 truly represents \"no damage\"\n",
    "  - May need to be treated as a categorical rather than boolean feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0273ca",
   "metadata": {},
   "source": [
    "<a id=\"categorical-features\"></a>\n",
    "### Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4432c021",
   "metadata": {},
   "source": [
    "<a id=\"check-categorical-features-consistency\"></a>\n",
    "#### Check Categorical Features Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095f1162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of categorical features\n",
    "cat_cols = ['Brand', 'model', 'fuelType', 'transmission']\n",
    "\n",
    "# Identify outlier examples in categorical features\n",
    "cat_outliers_examples = {col: df[col].value_counts().tail(10).index for col in cat_cols}\n",
    "\n",
    "# Display the outlier examples\n",
    "pd.DataFrame(cat_outliers_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa457064",
   "metadata": {},
   "source": [
    "<a id=\"categorical-features-summary\"></a>\n",
    "#### Categorical Features Summary\n",
    "- Initial analysis reveals significant data quality issues across all categorical columns\n",
    "- No standardization in categorical features, with multiple variations of the same values (different spellings, capitalizations)\n",
    "- Solution: We will implement string distance-based standardization using the `thefuzz` library to clean and standardize these features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ab1e0a",
   "metadata": {},
   "source": [
    "<a id=\"numerical-features\"></a>\n",
    "### Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6a26e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict of numerical features\n",
    "num_cols = {\n",
    "    'price': 'continuous',\n",
    "    'mileage': 'continuous',\n",
    "    'tax': 'continuous',\n",
    "    'mpg': 'continuous',\n",
    "    'paintQuality%': 'continuous',\n",
    "    'engineSize': 'continuous',\n",
    "    'year': 'discrete',\n",
    "    'previousOwners': 'discrete'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15d6909",
   "metadata": {},
   "source": [
    "<a id=\"plots\"></a>\n",
    "#### Numerical Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d22957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot figures for numerical features and the target variable (price)\n",
    "plt.figure(figsize=(16, 10))\n",
    "for i, (col, var_type) in enumerate(num_cols.items(), 1):\n",
    "    plt.subplot(4, 2, i)\n",
    "\n",
    "    # Plot based on variable type\n",
    "    if var_type == 'continuous':\n",
    "        sns.histplot(data=df, x=col, kde=True, color=\"lightcoral\", bins=30)\n",
    "        plt.title(f\"Distribution of {col}\", fontsize=11)\n",
    "    elif var_type == 'discrete':\n",
    "        sns.countplot(data=df, x=col, color=\"lightcoral\")\n",
    "        plt.title(f\"Distribution of {col}\", fontsize=11)\n",
    "        plt.xticks(rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f095a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots for continuous numerical features and the target variable (price)\n",
    "continuous_cols = [col for col, var_type in num_cols.items() if var_type == 'continuous']\n",
    "plt.figure(figsize=(16, 10))\n",
    "for i, col in enumerate(continuous_cols, 1):\n",
    "    plt.subplot(3, 2, i)\n",
    "    sns.boxplot(data=df, x=col, color=\"lightblue\")\n",
    "    plt.title(f\"Distribution of {col}\", fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0576a8e8",
   "metadata": {},
   "source": [
    "<a id=\"analysis-of-numerical-distributions\"></a>\n",
    "#### Analysis of Numerical Distributions\n",
    "\n",
    "Key observations from the plots:\n",
    "- **Target Variable (Price)**:\n",
    "  - Highly right-skewed distribution\n",
    "  - Contains significant number of outliers in the upper range\n",
    "  - Most cars are concentrated in the lower price range\n",
    "\n",
    "- **Mileage**:\n",
    "  - Right-skewed distribution\n",
    "  - Large range from nearly new cars to high-mileage vehicles\n",
    "  - Some outliers in upper range suggesting possible data entry errors\n",
    "\n",
    "- **Tax**:\n",
    "  - Multiple peaks suggesting different tax bands\n",
    "  - Contains negative values which require investigation (possible tax benefits/rebates)\n",
    "  - Large number of outliers on both ends of the distribution\n",
    "\n",
    "- **MPG (Miles Per Gallon)**:\n",
    "  - Approximately normal distribution with slight right skew\n",
    "  - Some unrealistic extreme values that need cleaning\n",
    "  - Reasonable median around typical car efficiency ranges\n",
    "\n",
    "- **Paint Quality %**:\n",
    "  - Contains values above 100% which are logically impossible\n",
    "  - Left-skewed distribution suggesting optimistic ratings\n",
    "  - Requires standardization to 0-100 range\n",
    "\n",
    "- **Engine Size**:\n",
    "  - There are engine size with zero values which are not realistic (might indicate electric vehicles)\n",
    "  - Some unusual patterns that need investigation\n",
    "  - Contains outliers that may represent specialty vehicles\n",
    "\n",
    "- **Year**:\n",
    "  - Should be discrete but contains decimal values\n",
    "\n",
    "- **Previous Owners**:\n",
    "  - Should be integer but contains float values\n",
    "  - Right-skewed distribution as expected\n",
    "  - Maximum values need validation (unusually high number of previous owners)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f2c3cb",
   "metadata": {},
   "source": [
    "<a id=\"pre-processing\"></a>\n",
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0225083",
   "metadata": {},
   "source": [
    "<a id=\"functions\"></a>\n",
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6c0933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  general_cleaning(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Perform general data cleaning on the DataFrame.\n",
    "    \n",
    "    This function handles logical inconsistencies and data quality issues that\n",
    "    don't require statistical calculations (mean, median, etc.) to prevent data\n",
    "    leakage between training and validation sets.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame containing car data with columns:\n",
    "            Brand, model, year, transmission, fuelType, mileage, tax, mpg, \n",
    "            engineSize, paintQuality%, previousOwners, hasDamage.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: The cleaned DataFrame with logical inconsistencies resolved.\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # Set negative values to NaN for features that shouldn't be negative\n",
    "    for col in ['previousOwners', 'mileage', 'tax', 'mpg', 'engineSize']:\n",
    "        df.loc[df[col] < 0, col] = np.nan\n",
    "\n",
    "    for col in ['Brand', 'model', 'transmission', 'fuelType']:\n",
    "        df[col] = df[col].str.lower()\n",
    "        df[col] = df[col].replace('', np.nan)\n",
    "\n",
    "    # Remove decimal part from 'year'\n",
    "    df['year'] = np.floor(df['year']).astype('Int64')\n",
    "\n",
    "    # Remove decimal part from 'previousOwners'\n",
    "    df['previousOwners'] = np.floor(df['previousOwners']).astype('Int64')\n",
    "\n",
    "    # Ensure 'paintQuality%' is within 0-100\n",
    "    df.loc[(df['paintQuality%'] < 0) | (df['paintQuality%'] > 100), 'paintQuality%'] = np.nan\n",
    "\n",
    "    # Fill missing 'hasDamage' with 1\n",
    "    df['hasDamage'] = df['hasDamage'].fillna(1).astype('Int64')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48fcfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_categorical_col(series: pd.Series, \n",
    "                                standardised_cats: list[str], \n",
    "                                distance_threshold: int = 2) -> pd.Series:\n",
    "    \"\"\"Standardizes a categorical column using edit distance with a threshold.\n",
    "\n",
    "    1. Maps values to a standard category if they are a likely typo\n",
    "       (i.e., within the edit distance_threshold).\n",
    "    2. Keeps values that are already in the standard list.\n",
    "    3. Groups all other values that don't match into an 'other' bin.\n",
    "    \n",
    "    Args:\n",
    "        series (pd.Series): The categorical column to standardize.\n",
    "        standardised_cats (list[str]): The list of \"good\" categories to match against.\n",
    "        distance_threshold (int): The max edit distance to consider something a typo.\n",
    "                                  A value of 1 or 2 is recommended.\n",
    "                                  \n",
    "    Returns:\n",
    "        pd.Series: The standardized categorical column.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Get all unique non-null values from the series\n",
    "    unique_values = series.dropna().unique()\n",
    "    \n",
    "    # 2. Build the mapping dictionary\n",
    "    mapping = {}\n",
    "    \n",
    "    for x in unique_values:\n",
    "        x_str = str(x)\n",
    "        \n",
    "        # Check if it's already a perfect match\n",
    "        if x_str in standardised_cats:\n",
    "            mapping[x] = x_str\n",
    "            continue\n",
    "\n",
    "        # Find the closest match and its distance\n",
    "        distances = [nltk.edit_distance(x_str, cat) for cat in standardised_cats]\n",
    "        min_distance = np.min(distances)\n",
    "        \n",
    "        if min_distance <= distance_threshold:\n",
    "            closest_cat = standardised_cats[np.argmin(distances)]\n",
    "            mapping[x] = closest_cat\n",
    "        else:\n",
    "            mapping[x] = 'other'\n",
    "            \n",
    "    return series.map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9309596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_high_freq(series: pd.Series, percent_threshold: float = 0.02) -> list[str]:\n",
    "    \"\"\"Get categories that appear more than a dynamic percentage threshold.\n",
    "    \n",
    "    Args:\n",
    "        series (pd.Series): The categorical series to analyze.\n",
    "        percent_threshold (float): The minimum percentage of total rows a category\n",
    "                                   must have to be included (e.g., 0.01 for 1%).\n",
    "                                   \n",
    "    Returns:\n",
    "        list[str]: List of categories with frequency above the dynamic threshold.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the dynamic count threshold based on the percentage\n",
    "    dynamic_count_threshold = len(series) * percent_threshold\n",
    "    \n",
    "    value_counts = series.value_counts()\n",
    "    \n",
    "    # Use the *same logic* as before, but with the new dynamic threshold\n",
    "    high_freq_cats = value_counts[value_counts > dynamic_count_threshold].index.tolist()\n",
    "    \n",
    "    return high_freq_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1795c746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_upper_bound(series: pd.Series) -> float:\n",
    "    \"\"\"Calculates the upper outlier bound for a pandas Series.\"\"\"\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    return Q3 + (1.5 * IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3254edf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_outliers(series: pd.Series, \n",
    "                   upper_bound: float,\n",
    "                   lower_bound: float = 0.0, \n",
    "                   return_missing: bool = True) -> pd.Series:\n",
    "    \"\"\"Clean outliers in the Series based on specified bounds.\n",
    "\n",
    "    This function clips values outside the specified bounds or sets them to NaN.\n",
    "    \n",
    "    Args:\n",
    "        series (pd.Series): The input Series containing numerical data.\n",
    "        lower_bound (float): The lower bound for valid values.\n",
    "        upper_bound (float): The upper bound for valid values.\n",
    "        return_missing (bool): If True, set out-of-bound values to NaN.\n",
    "                              If False, clip values to the bounds.\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series: The cleaned Series with outliers handled.\n",
    "    \"\"\"\n",
    "    cleaned = series.copy()\n",
    "    \n",
    "    if return_missing:\n",
    "        # Set out-of-bound values to NaN\n",
    "        cleaned[(cleaned < lower_bound) | (cleaned > upper_bound)] = np.nan\n",
    "    else:\n",
    "        # Clip values to the specified bounds\n",
    "        cleaned = cleaned.clip(lower=lower_bound, upper=upper_bound)\n",
    "    \n",
    "    return cleaned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e972a0",
   "metadata": {},
   "source": [
    "<a id=\"split-data\"></a>\n",
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9194c9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply general cleaning to the entire dataset, before splitting\n",
    "df_cleaned = general_cleaning(df)\n",
    "\n",
    "# Split the cleaned data into training and validation sets\n",
    "X = df_cleaned.drop(columns=[\"price\"])   \n",
    "y = df_cleaned[\"price\"]\n",
    "del num_cols['price']\n",
    "continuous_cols.remove('price')\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Validation set size: {X_val.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2120027d",
   "metadata": {},
   "source": [
    "<a id=\"target-transformation\"></a>\n",
    "#### Target Transformation\n",
    "\n",
    "Apply log transformation to the target variable (price) to:\n",
    "- Reduce skewness in the distribution\n",
    "- Handle wide price ranges more effectively\n",
    "- Stabilize variance\n",
    "- Improve model performance\n",
    "\n",
    "We use `np.log1p()` which computes log(1 + x) to handle any zero values safely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203f83a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log transformation to target variable\n",
    "y_train_log = np.log1p(y_train)\n",
    "y_val_log = np.log1p(y_val)\n",
    "\n",
    "print(f\"Original y_train range: £{y_train.min():.2f} - £{y_train.max():.2f}\")\n",
    "print(f\"Log-transformed y_train range: {y_train_log.min():.4f} - {y_train_log.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce60cb3",
   "metadata": {},
   "source": [
    "<a id=\"outliers-and-missing-values\"></a>\n",
    "### Outliers and Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979b2a8e",
   "metadata": {},
   "source": [
    "<a id=\"categorical-outliers-and-missing-values\"></a>\n",
    "#### Categorical Outliers and Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b22f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map of categorical columns to their high frequency categories\n",
    "# Using only the categories in train\n",
    "high_freq_categories_by_col = {col: get_categories_high_freq(X_train[col]) for col in cat_cols}\n",
    "\n",
    "for col in cat_cols:\n",
    "    standardised_cats = high_freq_categories_by_col[col]\n",
    "    # process training data\n",
    "    X_train[col] = standardize_categorical_col(X_train[col], standardised_cats)\n",
    "    X_train[col] =X_train[col].fillna('other')\n",
    "    \n",
    "    # process validation data\n",
    "    X_val[col] = standardize_categorical_col(X_val[col], standardised_cats)\n",
    "    X_val[col] = X_val[col].fillna('other')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ce953d",
   "metadata": {},
   "source": [
    "<a id=\"numerical-outliers\"></a>\n",
    "#### Numerical Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5010863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mileage is right_skewed and extremes might exist\n",
    "mileage_upper_bound = X_train['mileage'].quantile(0.95)\n",
    "\n",
    "# Calculate upper bounds for each continuous numerical feature (excluding price - it's the target)\n",
    "continuous_cols_for_outliers = [col for col in continuous_cols if col != 'price']\n",
    "upper_bounds_num_cols = {col: calculate_upper_bound(X_train[col]) for col in continuous_cols_for_outliers}\n",
    "\n",
    "# Winsorization at 95th percentile for mileage\n",
    "X_train['mileage'] = clean_outliers(X_train['mileage'], \n",
    "                                     lower_bound=0, \n",
    "                                     upper_bound=mileage_upper_bound, \n",
    "                                     return_missing=False)\n",
    "\n",
    "X_val['mileage'] = clean_outliers(X_val['mileage'], \n",
    "                                     lower_bound=0, \n",
    "                                     upper_bound=mileage_upper_bound, \n",
    "                                     return_missing=False)\n",
    "\n",
    "# Turn outliers for remaining columns into np.nan\n",
    "for col in continuous_cols_for_outliers:\n",
    "    if col == 'mileage':\n",
    "        continue\n",
    "    upper_bound = upper_bounds_num_cols[col]\n",
    "    \n",
    "    X_train[col] = clean_outliers(X_train[col], upper_bound)\n",
    "    X_val[col] = clean_outliers(X_val[col], upper_bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacc0fcf",
   "metadata": {},
   "source": [
    "<a id=\"numerical-missing-values\"></a>\n",
    "#### Numerical Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bb8f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_num_cols = {col: X_train[col].median() for col in num_cols}\n",
    "\n",
    "for col in num_cols:\n",
    "    calc_median = median_num_cols[col]\n",
    "    X_train[col] = X_train[col].fillna(calc_median)\n",
    "    X_val[col] = X_val[col].fillna(calc_median)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f09a049",
   "metadata": {},
   "source": [
    "<a id=\"feature-engineering\"></a>\n",
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e266f94",
   "metadata": {},
   "source": [
    "<a id=\"encode-features\"></a>\n",
    "## Encode Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e2b379",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = cat_cols\n",
    "numeric_features = list(num_cols.keys()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824f3295",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "ohe_train = pd.DataFrame(\n",
    "    ohe.fit_transform(X_train[categorical_features]),\n",
    "    columns=ohe.get_feature_names_out(categorical_features),\n",
    "    index=X_train.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff40407f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_val = pd.DataFrame(\n",
    "    ohe.transform(X_val[categorical_features]),\n",
    "    columns=ohe.get_feature_names_out(categorical_features),\n",
    "    index=X_val.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6447bc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([X_train.drop(columns=categorical_features), ohe_train], axis=1)\n",
    "X_val   = pd.concat([X_val.drop(columns=categorical_features),   ohe_val],   axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55ba824",
   "metadata": {},
   "source": [
    "<a id=\"normalize-features\"></a>\n",
    "## Normalize Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faece80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "# Get numerical column names (exclude OHE columns)\n",
    "numerical_cols = list(num_cols.keys())\n",
    "\n",
    "# Fit scaler on training data and transform both sets\n",
    "X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n",
    "X_val[numerical_cols] = scaler.transform(X_val[numerical_cols])\n",
    "\n",
    "print(f\"Normalized {len(numerical_cols)} numerical features\")\n",
    "print(f\"Feature names: {numerical_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d6d302",
   "metadata": {},
   "source": [
    "<a id=\"correlation-matrix\"></a>\n",
    "### Correlation Matrix for Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35af957d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "corr = X_train[list(num_cols.keys())].corr(method=\"pearson\")\n",
    "sns.heatmap(data=corr, annot=True, )\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456e15c9",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "- Most correlations are weak, meaning variables are largely independent\n",
    "\n",
    "---\n",
    "\n",
    "##### Insights\n",
    "\n",
    "**Mileage vs Year (-0.68)**\n",
    "- Newer cars (higher `year`) tend to have lower mileage.\n",
    "- These two features describe similar aspects of car age/usage.\n",
    "\n",
    "**Tax vs mpg (-0.51)**\n",
    "- Cars with better fuel efficiency (higher mpg) tend to have lower taxes, likely due to emissions-based tax systems.\n",
    "\n",
    "**Tax vs Year (0.35)**\n",
    "- **Weak positive correlation.**\n",
    "- Newer cars may have slightly higher taxes, possibly reflecting newer model valuations or updated emission standards.\n",
    "\n",
    "**Mileage vs mpg (0.25)**\n",
    "- **Weak positive correlation.**\n",
    "- Slightly counterintuitive — could suggest that cars with better mpg are driven more (used as daily vehicles).\n",
    "\n",
    "\n",
    "**Other variables**\n",
    "- `paintQuality%`, `engineSize`, and `previousOwners` show very low correlations (~ 0) with other variables.  \n",
    "- These likely capture unique information and should be kept for modeling.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f115e4",
   "metadata": {},
   "source": [
    "<a id=\"model-training\"></a>\n",
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfc3b63",
   "metadata": {},
   "source": [
    "<a id=\"baseline-model-training\"></a>\n",
    "## Baseline Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab6b358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model on log-transformed target\n",
    "baseline_model = Ridge(fit_intercept = False)\n",
    "baseline_model.fit(X_train, y_train_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ebdfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions and metrics\n",
    "y_train_pred = np.expm1(baseline_model.predict(X_train))\n",
    "y_val_pred = np.expm1(baseline_model.predict(X_val))\n",
    "\n",
    "mae_train, mae_val = mean_absolute_error(y_train, y_train_pred), mean_absolute_error(y_val, y_val_pred)\n",
    "rmse_train, rmse_val = root_mean_squared_error(y_train, y_train_pred), root_mean_squared_error(y_val, y_val_pred)\n",
    "r2_train, r2_val = r2_score(y_train, y_train_pred), r2_score(y_val, y_val_pred)\n",
    "\n",
    "print(f\"Baseline - Train: MAE=£{mae_train:.2f}, RMSE=£{rmse_train:.2f}, R²={r2_train:.4f}\")\n",
    "print(f\"Baseline - Val:   MAE=£{mae_val:.2f}, RMSE=£{rmse_val:.2f}, R²={r2_val:.4f}\")\n",
    "\n",
    "# Store baseline metrics for comparison\n",
    "baseline_metrics = {\n",
    "    'mae_train': mae_train,\n",
    "    'mae_val': mae_val,\n",
    "    'rmse_train': rmse_train,\n",
    "    'rmse_val': rmse_val,\n",
    "    'r2_train': r2_train,\n",
    "    'r2_val': r2_val\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f05367",
   "metadata": {},
   "source": [
    "<a id=\"experiment-with-different-models\"></a>\n",
    "## Experiment with Different Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06aa530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_model(model, name, baseline_metrics=None):\n",
    "    \"\"\"\n",
    "    Fit model on log-transformed target and evaluate performance.\n",
    "    \n",
    "    Args:\n",
    "        model: sklearn model instance to train and evaluate\n",
    "        name: str, display name for the model\n",
    "        baseline_metrics: dict, optional baseline metrics for comparison\n",
    "                         (keys: 'mae_val', 'r2_val', etc.)\n",
    "    \n",
    "    Returns:\n",
    "        dict: metrics containing MAE, RMSE, and R² for train and validation sets\n",
    "    \"\"\"\n",
    "    model.fit(X_train, y_train_log)\n",
    "    \n",
    "    y_train_pred = np.expm1(model.predict(X_train))\n",
    "    y_val_pred = np.expm1(model.predict(X_val))\n",
    "    \n",
    "    mae_t, mae_v = mean_absolute_error(y_train, y_train_pred), mean_absolute_error(y_val, y_val_pred)\n",
    "    rmse_t, rmse_v = root_mean_squared_error(y_train, y_train_pred), root_mean_squared_error(y_val, y_val_pred)\n",
    "    \n",
    "    print(f\"\\n{name}\")\n",
    "    print(f\"  Train: MAE=£{mae_t:.2f}, RMSE=£{rmse_t:.2f}\")\n",
    "    print(f\"  Val:   MAE=£{mae_v:.2f}, RMSE=£{rmse_v:.2f}\")\n",
    "    \n",
    "    if baseline_metrics:\n",
    "        mae_diff = mae_v - baseline_metrics['mae_val']\n",
    "        status = \"Better\" if mae_diff < 0 else \"Worse\" if mae_diff > 0 else \"Same\"\n",
    "        print(f\"  vs Baseline: MAE {mae_diff:+.2f} - {status}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a45d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = train_evaluate_model(\n",
    "    Lasso(alpha=0.1),\n",
    "    \"Lasso Regression\",\n",
    "    baseline_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af106161",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression_model = train_evaluate_model(\n",
    "    LinearRegression(),\n",
    "    \"Linear Regression\",\n",
    "    baseline_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8205f384",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_model_raw = train_evaluate_model(\n",
    "    RandomForestRegressor(random_state=SEED),\n",
    "    \"Random Forest Regressor (raw)\",\n",
    "    baseline_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6b7a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = random_forest_model_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472f9e80",
   "metadata": {},
   "source": [
    "### Model Selection Conclusion\n",
    "\n",
    "The selected model shows some signs of overfitting, with better training performance compared to validation (Train MAE < Val MAE). However even on the validation set, the metrics demonstrate good performance, compared to the baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb360862",
   "metadata": {},
   "source": [
    "<a id=\"predictions\"></a>\n",
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb33e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('data/test.csv').set_index('carID')\n",
    "test_df_cleaned = general_cleaning(test_df)\n",
    "test_df_cleaned.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b490b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cat_cols:\n",
    "    standardised_cats = high_freq_categories_by_col[col]\n",
    "    # process training data\n",
    "    test_df_cleaned[col] = standardize_categorical_col(test_df_cleaned[col], standardised_cats)\n",
    "    test_df_cleaned[col] = test_df_cleaned[col].fillna('other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fbdef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply mileage winsorization using the TRAINING upper bound\n",
    "test_df_cleaned['mileage'] = clean_outliers(\n",
    "    test_df_cleaned['mileage'], \n",
    "    lower_bound=0, \n",
    "    upper_bound=mileage_upper_bound,  # From training data\n",
    "    return_missing=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d60fc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn outliers for remaining columns into np.nan (using same list as training)\n",
    "continuous_cols_for_outliers = [col for col in continuous_cols if col != 'price']\n",
    "for col in continuous_cols_for_outliers:\n",
    "    if col == 'mileage':\n",
    "        continue\n",
    "    upper_bound = upper_bounds_num_cols[col]\n",
    "    \n",
    "    test_df_cleaned[col] = clean_outliers(test_df_cleaned[col], upper_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1285f601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values using TRAINING medians\n",
    "for col in num_cols:\n",
    "    calc_median = median_num_cols[col]  # Get the median from training data\n",
    "    test_df_cleaned[col] = test_df_cleaned[col].fillna(calc_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8d3ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_test = pd.DataFrame(\n",
    "    ohe.transform(test_df_cleaned[categorical_features]),\n",
    "    columns=ohe.get_feature_names_out(categorical_features),\n",
    "    index=test_df_cleaned.index\n",
    ")\n",
    "\n",
    "test_df_cleaned = pd.concat([test_df_cleaned.drop(columns=categorical_features), ohe_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf504947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply StandardScaler normalization using training statistics\n",
    "test_df_cleaned[numerical_cols] = scaler.transform(test_df_cleaned[numerical_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51703e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions (in log space) and inverse transform to original scale\n",
    "test_df_cleaned['price'] = np.expm1(model.predict(test_df_cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cb6ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_cleaned['price'].to_csv('data/test_predictions.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
