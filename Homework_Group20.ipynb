{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "891cffd7",
   "metadata": {},
   "source": [
    "# Machine Learning Project\n",
    "\n",
    "## Table of Contents\n",
    "- [Import Data](#import-data)\n",
    "- [Data Exploration](#data-exploration)\n",
    "  - [Boolean Features](#boolean-features)\n",
    "  - [Categorical Features](#categorical-features)\n",
    "  - [Numerical Features](#numerical-features)\n",
    "    - [Numerical Plots](#numerical-plots)\n",
    "- [Pre-processing](#pre-processing)\n",
    "  - [Functions](#functions)\n",
    "  - [Missing Values](#missing-values)\n",
    "  - [String Distance](#string-distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47693163",
   "metadata": {},
   "source": [
    "<a id=\"import-data\"></a>\n",
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9c6c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# to calculate distance between strings\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6468f612",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv').set_index('carID')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc133ea3-1850-4386-ab5e-6db9950c48f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a5a91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_duplicated_ids = df.index.duplicated().sum()\n",
    "print(f'Number of duplicated carIDs: {num_duplicated_ids}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c607395",
   "metadata": {},
   "source": [
    "#### Import Data Summary\n",
    "- Dataset loaded successfully with `carID` as the index\n",
    "- There are no duplicate entries in carID\n",
    "- The dataset contains information about cars including both numerical features (price, mileage, tax, etc.) and categorical features (brand, model, transmission, etc.)\n",
    "- Initial inspection shows multiple features that will require preprocessing:\n",
    "  - Numerical features that need cleaning (negative values, outliers)\n",
    "  - Categorical features that need standardization\n",
    "  - Presence of missing values in several columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3db49c",
   "metadata": {},
   "source": [
    "<a id=\"data-exploration\"></a>\n",
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb63f270",
   "metadata": {},
   "source": [
    "<a id=\"boolean-features\"></a>\n",
    "### Boolean Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e5927d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hasDamage'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb51349",
   "metadata": {},
   "source": [
    "#### Boolean Features Analysis\n",
    "\n",
    "Key observations about `hasDamage` feature:\n",
    "- Only contains binary values (0) and NaN\n",
    "- No instances of value 1 found, suggesting potential data collection issues\n",
    "- May indicate:\n",
    "  - Cars with damage not being listed\n",
    "  - System default setting of 0 for non-damaged cars\n",
    "  - Incomplete damage assessment process\n",
    "- Requires special handling in preprocessing:\n",
    "  - Consider treating NaN as a separate category\n",
    "  - Validate if 0 truly represents \"no damage\"\n",
    "  - May need to be treated as a categorical rather than boolean feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0273ca",
   "metadata": {},
   "source": [
    "<a id=\"categorical-features\"></a>\n",
    "### Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4432c021",
   "metadata": {},
   "source": [
    "#### Check Categorical Features Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095f1162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of categorical features\n",
    "cat_cols = ['Brand', 'model', 'fuelType', 'transmission']\n",
    "\n",
    "# Identify outlier examples in categorical features\n",
    "cat_outliers_examples = {col: df[col].value_counts().tail(10).index for col in cat_cols}\n",
    "\n",
    "# Display the outlier examples\n",
    "pd.DataFrame(cat_outliers_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa457064",
   "metadata": {},
   "source": [
    "#### Categorical Features Summary\n",
    "- Initial analysis reveals significant data quality issues across all categorical columns\n",
    "- No standardization in categorical features, with multiple variations of the same values (different spellings, capitalizations)\n",
    "- Solution: We will implement string distance-based standardization using the `thefuzz` library to clean and standardize these features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ab1e0a",
   "metadata": {},
   "source": [
    "<a id=\"numerical-features\"></a>\n",
    "### Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6a26e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict of numerical features\n",
    "num_cols = {\n",
    "    'price': 'continuous',\n",
    "    'mileage': 'continuous',\n",
    "    'tax': 'continuous',\n",
    "    'mpg': 'continuous',\n",
    "    'paintQuality%': 'continuous',\n",
    "    'engineSize': 'continuous',\n",
    "    'year': 'discrete',\n",
    "    'previousOwners': 'discrete'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15d6909",
   "metadata": {},
   "source": [
    "<a id=\"plots\"></a>\n",
    "#### Numerical Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d22957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot figures for numerical features and the target variable (price)\n",
    "plt.figure(figsize=(16, 10))\n",
    "for i, (col, var_type) in enumerate(num_cols.items(), 1):\n",
    "    plt.subplot(4, 2, i)\n",
    "\n",
    "    # Plot based on variable type\n",
    "    if var_type == 'continuous':\n",
    "        sns.histplot(data=df, x=col, kde=True, color=\"lightcoral\", bins=30)\n",
    "        plt.title(f\"Distribution of {col}\", fontsize=11)\n",
    "    elif var_type == 'discrete':\n",
    "        sns.countplot(data=df, x=col, color=\"lightcoral\")\n",
    "        plt.title(f\"Distribution of {col}\", fontsize=11)\n",
    "        plt.xticks(rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f095a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots for continuous numerical features and the target variable (price)\n",
    "continuous_cols = [col for col, var_type in num_cols.items() if var_type == 'continuous']\n",
    "plt.figure(figsize=(16, 10))\n",
    "for i, col in enumerate(continuous_cols, 1):\n",
    "    plt.subplot(3, 2, i)\n",
    "    sns.boxplot(data=df, x=col, color=\"lightblue\")\n",
    "    plt.title(f\"Distribution of {col}\", fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0576a8e8",
   "metadata": {},
   "source": [
    "#### Analysis of Numerical Distributions\n",
    "\n",
    "Key observations from the plots:\n",
    "- **Target Variable (Price)**:\n",
    "  - Highly right-skewed distribution\n",
    "  - Contains significant number of outliers in the upper range\n",
    "  - Most cars are concentrated in the lower price range\n",
    "\n",
    "- **Mileage**:\n",
    "  - Right-skewed distribution\n",
    "  - Large range from nearly new cars to high-mileage vehicles\n",
    "  - Some outliers in upper range suggesting possible data entry errors\n",
    "\n",
    "- **Tax**:\n",
    "  - Multiple peaks suggesting different tax bands\n",
    "  - Contains negative values which require investigation (possible tax benefits/rebates)\n",
    "  - Large number of outliers on both ends of the distribution\n",
    "\n",
    "- **MPG (Miles Per Gallon)**:\n",
    "  - Approximately normal distribution with slight right skew\n",
    "  - Some unrealistic extreme values that need cleaning\n",
    "  - Reasonable median around typical car efficiency ranges\n",
    "\n",
    "- **Paint Quality %**:\n",
    "  - Contains values above 100% which are logically impossible\n",
    "  - Left-skewed distribution suggesting optimistic ratings\n",
    "  - Requires standardization to 0-100 range\n",
    "\n",
    "- **Engine Size**:\n",
    "  - Multi-modal distribution indicating common engine sizes\n",
    "  - Some unusual patterns that need investigation\n",
    "  - Contains outliers that may represent specialty vehicles\n",
    "\n",
    "- **Year**:\n",
    "  - Should be discrete but contains decimal values\n",
    "  - Most cars are from recent years (2015-2023)\n",
    "  - Some unrealistic values (pre-1950 or post-2025)\n",
    "\n",
    "- **Previous Owners**:\n",
    "  - Should be integer but contains float values\n",
    "  - Right-skewed distribution as expected\n",
    "  - Maximum values need validation (unusually high number of previous owners)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f2c3cb",
   "metadata": {},
   "source": [
    "<a id=\"pre-processing\"></a>\n",
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0225083",
   "metadata": {},
   "source": [
    "<a id=\"functions\"></a>\n",
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6c0933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  general_cleaning(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Perform general data cleaning on the DataFrame.\n",
    "    \n",
    "    This function handles logical inconsistencies and data quality issues that\n",
    "    don't require statistical calculations (mean, median, etc.) to prevent data\n",
    "    leakage between training and validation sets.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame containing car data with columns:\n",
    "            Brand, model, year, transmission, fuelType, mileage, tax, mpg, \n",
    "            engineSize, paintQuality%, previousOwners, hasDamage.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: The cleaned DataFrame with logical inconsistencies resolved.\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # Set negative values to NaN for features that shouldn't be negative\n",
    "    for col in ['previousOwners', 'mileage', 'tax', 'mpg', 'engineSize']:\n",
    "        df.loc[df[col] < 0, col] = np.nan\n",
    "\n",
    "    for col in ['Brand', 'model', 'transmission', 'fuelType']:\n",
    "        df[col] = df[col].str.lower()\n",
    "        df[col] = df[col].replace('', np.nan)\n",
    "\n",
    "    # Remove decimal part from 'year'\n",
    "    df['year'] = np.floor(df['year'])\n",
    "\n",
    "    # Remove decimal part from 'previousOwners'\n",
    "    df['previousOwners'] = np.floor(df['previousOwners'])\n",
    "\n",
    "    # Ensure 'paintQuality%' is within 0-100\n",
    "    df.loc[(df['paintQuality%'] < 0) | (df['paintQuality%'] > 100), 'paintQuality%'] = np.nan\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48fcfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_categorical_col(series: pd.Series, standardised_cats: list[str]) -> pd.Series:\n",
    "    \"\"\"Standardizes the entries of a categorical column based on a list of standard categories.\n",
    "    The function uses edit distance to find the closest match from the standard categories.\n",
    "    for example, \"Vokswagen\" would be standardized to \"Volkswagen\".\n",
    "    \n",
    "    Args:\n",
    "        series (pd.Series): The categorical column to standardize.\n",
    "        standardised_cats (list[str]): List of standard category names.\n",
    "        \n",
    "    Returns:\n",
    "        pd.Series: The standardized categorical column.\n",
    "    \"\"\"\n",
    "    \n",
    "    def find_closest_match(x):\n",
    "        if pd.isnull(x):\n",
    "            return np.nan\n",
    "        distances = [nltk.edit_distance(str(x), cat) for cat in standardised_cats]\n",
    "        return standardised_cats[np.argmin(distances)]\n",
    "    \n",
    "    return series.apply(find_closest_match)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9309596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_high_freq(series: pd.Series, threshold: int = 100) -> list[str]:\n",
    "    \"\"\"Get categories in a categorical column that appear more than a specified threshold.\n",
    "    \n",
    "    Args:\n",
    "        series (pd.Series): The categorical series to analyze.\n",
    "        threshold (int): The minimum frequency for a category to be included.\n",
    "        \n",
    "    Returns:\n",
    "        list[str]: List of categories with frequency above the threshold.\n",
    "    \"\"\"\n",
    "    value_counts = series.value_counts()\n",
    "    high_freq_cats = value_counts[value_counts > threshold].index.tolist()\n",
    "    return high_freq_cats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9194c9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"price\"])   \n",
    "y = df[\"price\"]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Validation set size: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637e5e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_clean = X_train.copy()\n",
    "X_val_clean = X_val.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009118e1",
   "metadata": {},
   "source": [
    "<a id=\"missing-values\"></a>\n",
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57589a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_general = [\"tax\", \"previousOwners\", \"paintQuality%\", \"engineSize\"]\n",
    "\n",
    "median_imputer = SimpleImputer(strategy=\"median\")\n",
    "X_train_clean[num_general] = median_imputer.fit_transform(X_train_clean[num_general])\n",
    "X_val_clean[num_general] = median_imputer.transform(X_val_clean[num_general])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7641491c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"mpg\" in X_train_clean.columns and \"fuelType\" in X_train_clean.columns:\n",
    "    mpg_group_medians = X_train_clean.groupby(\"fuelType\")[\"mpg\"].median()\n",
    "\n",
    "    def impute_mpg(row):\n",
    "        if pd.isnull(row[\"mpg\"]):\n",
    "            return mpg_group_medians.get(row[\"fuelType\"], X_train_clean[\"mpg\"].median())\n",
    "        return row[\"mpg\"]\n",
    "\n",
    "    X_train_clean[\"mpg\"] = X_train_clean.apply(impute_mpg, axis=1)\n",
    "    X_val_clean[\"mpg\"] = X_val_clean.apply(impute_mpg, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f7eece",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_summary_train = X_train_clean.isnull().sum()\n",
    "missing_summary_val = X_val_clean.isnull().sum()\n",
    "\n",
    "print(\"Remaining missing values (train):\", missing_summary_train.sum())\n",
    "print(\"Remaining missing values (validation):\", missing_summary_val.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64917c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_missing = X_train_clean.isnull().sum()\n",
    "remaining_missing = remaining_missing[remaining_missing > 0]\n",
    "print(\"Columns with remaining NaN values:\")\n",
    "display(remaining_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c931a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_missing = X_train_clean.isnull().sum()\n",
    "remaining_missing = remaining_missing[remaining_missing > 0]\n",
    "print(\"Columns with remaining NaN values:\")\n",
    "display(remaining_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f021430",
   "metadata": {},
   "source": [
    "### Numeric Features\n",
    "\n",
    "- carID\n",
    "- year\n",
    "- price\n",
    "- mileage\n",
    "- tax\n",
    "- mpg\n",
    "- engineSize\n",
    "- paintQuality\n",
    "- previousOwners\n",
    "- hasDamage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe06a535",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if there are negative features that should not be negative\n",
    "numeric_features = X_train_clean.select_dtypes(include=['int64', 'float64']).columns\n",
    "negative_values = {}\n",
    "for col in numeric_features:\n",
    "    negative_count = (X_train_clean[col] < 0).sum()\n",
    "    if negative_count > 0:\n",
    "        negative_values[col] = negative_count   \n",
    "\n",
    "for v in negative_values:\n",
    "    print(f\"Feature '{v}' has {negative_values[v]} negative values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b96ea6",
   "metadata": {},
   "source": [
    "### Strategy:\n",
    "\n",
    "- Change negative values to `NaN`\n",
    "- Remove extreme outliers\n",
    "- Impute using the appropriate method (median, mode, or group-based)\n",
    "- Convert the column back to integer type if applicable\n",
    "\n",
    "Note: Still have to choose what method to use to change the missing values in the feature \"hasDamage\" before I can switch from float to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b467aa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_fix = list(negative_values.keys())\n",
    "\n",
    "for feature in cols_to_fix:\n",
    "    X_train_clean.loc[X_train_clean[feature] < 0, feature] = np.nan\n",
    "    X_val_clean.loc[X_val_clean[feature] < 0, feature] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8891fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in X_train_clean.select_dtypes(include=['int64', 'float64']).columns:\n",
    "    q1 = X_train_clean[col].quantile(0.25)\n",
    "    q3 = X_train_clean[col].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower_lim = q1 - (1.5 * iqr)\n",
    "    upper_lim = q3 + (1.5 * iqr)\n",
    "    X_train_clean[col] = X_train_clean[col].mask((X_train_clean[col] < lower_lim) | (X_train_clean[col] > upper_lim), np.nan)\n",
    "    X_val_clean[col] = X_val_clean[col].mask((X_val_clean[col] < lower_lim) | (X_val_clean[col] > upper_lim), np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8e8be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in cols_to_fix:\n",
    "    median_value = X_train_clean[feature].median()\n",
    "    X_train_clean[feature].fillna(median_value, inplace=True)\n",
    "    X_val_clean[feature].fillna(median_value, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e6ca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_cols = ['year', 'previousOwners']\n",
    "\n",
    "median_year = X_train_clean['year'].median()\n",
    "X_train_clean['year'].fillna(median_year, inplace=True)\n",
    "X_val_clean['year'].fillna(median_year, inplace=True)\n",
    "\n",
    "for feature in int_cols:\n",
    "    X_train_clean[feature] = X_train_clean[feature].astype(int)\n",
    "    X_val_clean[feature] = X_val_clean[feature].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f72094e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_clean[\"year\"] < 1950).sum() and (X_train_clean[\"year\"] > 2025).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b97892",
   "metadata": {},
   "source": [
    "Final check for nan values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a391910c",
   "metadata": {},
   "source": [
    "Check for other strange values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e3fcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_clean.loc[(X_train_clean['paintQuality%'] > 100), 'paintQuality%'] = np.nan\n",
    "X_val_clean.loc[(X_val_clean['paintQuality%'] > 100), 'paintQuality%'] = np.nan\n",
    "\n",
    "median_paint = X_train_clean['paintQuality%'].median()\n",
    "X_train_clean['paintQuality%'].fillna(median_paint, inplace=True)\n",
    "X_val_clean['paintQuality%'].fillna(median_paint, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a97bf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_counts = X_train_clean.isnull().sum()\n",
    "missing_percent = (missing_counts / len(df)) * 100\n",
    "\n",
    "missing_summary = pd.DataFrame({\n",
    "    \"Missing Count\": missing_counts,\n",
    "    \"Missing %\": missing_percent.round(2)\n",
    "}).sort_values(by=\"Missing %\", ascending=False)\n",
    "\n",
    "missing_summary[missing_summary[\"Missing Count\"] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66054541",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = X_train_clean.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "\n",
    "outlier_summary = []\n",
    "\n",
    "for col in num_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = X_train_clean[(X_train_clean[col] < lower_bound) | (X_train_clean[col] > upper_bound)]\n",
    "    \n",
    "    outlier_summary.append({\n",
    "        \"Feature\": col,\n",
    "        \"Lower Bound\": round(lower_bound, 2),\n",
    "        \"Upper Bound\": round(upper_bound, 2),\n",
    "        \"Outlier Count\": len(outliers),\n",
    "        \"Outlier %\": round(len(outliers) / len(df) * 100, 2)\n",
    "    })\n",
    "\n",
    "outlier_df = pd.DataFrame(outlier_summary).sort_values(by=\"Outlier %\", ascending=False)\n",
    "outlier_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
