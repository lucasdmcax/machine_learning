{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "891cffd7",
   "metadata": {},
   "source": [
    "# Machine Learning Project\n",
    "\n",
    "## Table of Contents\n",
    "- [Import Data](#import-data)\n",
    "  - [Import Data Summary](#import-data-summary)\n",
    "- [Data Exploration](#data-exploration)\n",
    "  - [Boolean Features](#boolean-features)\n",
    "    - [Boolean Features Analysis](#boolean-features-analysis)\n",
    "  - [Categorical Features](#categorical-features)\n",
    "    - [Check Categorical Features Consistency](#check-categorical-features-consistency)\n",
    "    - [Categorical Features Summary](#categorical-features-summary)\n",
    "  - [Numerical Features](#numerical-features)\n",
    "    - [Numerical Plots](#plots)\n",
    "    - [Analysis of Numerical Distributions](#analysis-of-numerical-distributions)\n",
    "- [Pre-processing](#pre-processing)\n",
    "  - [Summary of Preprocessing Pipeline](#preprocessing-pipeline-summary)\n",
    "  - [Data Preparation](#data-preparation)\n",
    "    - [Correlation Analysis](#correlation-analysis)\n",
    "- [Model Training](#model-training)\n",
    "  - [Model Selection with Cross-Validation](#model-selection-with-cv)\n",
    "  - [Quick Baseline Model](#quick-baseline-model)\n",
    "  - [Experiment Algorithms](#experiment-algorithms)\n",
    "- [Predictions](#predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd1cee6",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "### Context:\n",
    "This project addresses car price prediction—a fundamental regression task in machine learning. The dataset includes features spanning categorical attributes (brand, model, transmission type, fuel type) and numerical characteristics (mileage, engine size, tax, MPG, paint quality). This problem is relevant for automotive valuations, insurance pricing, and market analysis.\n",
    "\n",
    "### Objectives:\n",
    "The primary goals were to:\n",
    "1.  Systematically explore and preprocess complex, real-world automotive data containing missing values, outliers, and inconsistencies.\n",
    "2.  Develop a robust preprocessing pipeline that prevents data leakage through proper fold-wise application.\n",
    "3.  Benchmark multiple regression algorithms with hyperparameter tuning via cross-validation.\n",
    "4.  Identify the most influential features through importance analysis.\n",
    "\n",
    "### Methodology:\n",
    "Data exploration revealed categorical inconsistencies (typos, spacing variations) and numerical anomalies (negative values, out-of-range percentages). The preprocessing pipeline incorporated:\n",
    "* General cleaning\n",
    "* Categorical standardization using edit distance\n",
    "* Outlier handling via IQR method\n",
    "* Imputation with training-set medians\n",
    "* One-hot encoding\n",
    "* Feature normalization\n",
    "\n",
    "\n",
    "### Main Results:\n",
    "**Ridge regression** with log-transformed targets established a baseline (MAE on validation). Feature importance revealed that `mileage`, `year`, and `engine size` are primary predictors, while `hasDamage` and certain categorical overflow categories contributed minimal signal. The hyperparameter tuning identified optimal configurations for each algorithm, with **ensemble methods** generally outperforming linear approaches. Cross-validation metrics tracked both training and validation performance to detect overfitting.\n",
    "\n",
    "### Conclusions:\n",
    "The project demonstrates that systematic preprocessing and ensemble approaches significantly improve prediction accuracy. Feature engineering and selection based on importance analysis reduced model complexity while preserving predictive power, supporting the principle that data quality and feature selection are as critical as algorithm choice in regression tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47693163",
   "metadata": {},
   "source": [
    "<a id=\"import-data\"></a>\n",
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9c6c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats import loguniform, randint\n",
    "from model_training_utils import (\n",
    "    general_cleaning,\n",
    "    preprocess_data,\n",
    "    cross_validate_with_tuning,\n",
    "    preprocess_test_data,\n",
    "    get_feature_importance\n",
    ")\n",
    "\n",
    "SEED = 42\n",
    "LOG_TARGET = True\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6468f612",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv').set_index('carID')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc133ea3-1850-4386-ab5e-6db9950c48f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a5a91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_duplicated_ids = df.index.duplicated().sum()\n",
    "print(f'Number of duplicated carIDs: {num_duplicated_ids}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c607395",
   "metadata": {},
   "source": [
    "<a id=\"import-data-summary\"></a>\n",
    "#### Import Data Summary\n",
    "- Dataset loaded successfully with `carID` as the index\n",
    "- There are no duplicate entries in carID\n",
    "- The dataset contains information about cars including both numerical features (price, mileage, tax, etc.) and categorical features (brand, model, transmission, etc.)\n",
    "- Initial inspection shows multiple features that will require preprocessing:\n",
    "  - Numerical features that need cleaning (negative values, outliers)\n",
    "  - Categorical features that need standardization\n",
    "  - Presence of missing values in several columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3db49c",
   "metadata": {},
   "source": [
    "<a id=\"data-exploration\"></a>\n",
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb63f270",
   "metadata": {},
   "source": [
    "<a id=\"boolean-features\"></a>\n",
    "### Boolean Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e5927d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hasDamage'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb51349",
   "metadata": {},
   "source": [
    "<a id=\"boolean-features-analysis\"></a>\n",
    "#### Boolean Features Analysis\n",
    "\n",
    "Key observations about `hasDamage` feature:\n",
    "- Only contains binary values (0) and NaN\n",
    "- No instances of value 1 found, suggesting potential data collection issues\n",
    "- May indicate:\n",
    "  - Cars with damage not being listed\n",
    "  - System default setting of 0 for non-damaged cars\n",
    "  - Incomplete damage assessment process\n",
    "- Requires special handling in preprocessing:\n",
    "  - Consider treating NaN as a separate category\n",
    "  - Validate if 0 truly represents \"no damage\"\n",
    "  - May need to be treated as a categorical rather than boolean feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0273ca",
   "metadata": {},
   "source": [
    "<a id=\"categorical-features\"></a>\n",
    "### Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4432c021",
   "metadata": {},
   "source": [
    "<a id=\"check-categorical-features-consistency\"></a>\n",
    "#### Check Categorical Features Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095f1162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of categorical features\n",
    "cat_cols = ['Brand', 'model', 'fuelType', 'transmission']\n",
    "\n",
    "# Identify outlier examples in categorical features\n",
    "cat_outliers_examples = {col: df[col].value_counts().tail(10).index for col in cat_cols}\n",
    "\n",
    "# Display the outlier examples\n",
    "pd.DataFrame(cat_outliers_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa457064",
   "metadata": {},
   "source": [
    "<a id=\"categorical-features-summary\"></a>\n",
    "#### Categorical Features Summary\n",
    "- Initial analysis reveals significant data quality issues across all categorical columns\n",
    "- No standardization in categorical features, with multiple variations of the same values (different spellings, capitalizations)\n",
    "- Solution: We will implement string distance-based standardization using the `nltk` library to clean and standardize these features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ab1e0a",
   "metadata": {},
   "source": [
    "<a id=\"numerical-features\"></a>\n",
    "### Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6a26e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict of numerical features\n",
    "num_cols = {\n",
    "    'price': 'continuous',\n",
    "    'mileage': 'continuous',\n",
    "    'tax': 'continuous',\n",
    "    'mpg': 'continuous',\n",
    "    'paintQuality%': 'continuous',\n",
    "    'engineSize': 'continuous',\n",
    "    'year': 'discrete',\n",
    "    'previousOwners': 'discrete'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15d6909",
   "metadata": {},
   "source": [
    "<a id=\"plots\"></a>\n",
    "#### Numerical Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d22957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot figures for numerical features and the target variable (price)\n",
    "plt.figure(figsize=(16, 10))\n",
    "for i, (col, var_type) in enumerate(num_cols.items(), 1):\n",
    "    plt.subplot(4, 2, i)\n",
    "\n",
    "    # Plot based on variable type\n",
    "    if var_type == 'continuous':\n",
    "        sns.histplot(data=df, x=col, kde=True, color=\"lightcoral\", bins=30)\n",
    "        plt.title(f\"Distribution of {col}\", fontsize=11)\n",
    "    elif var_type == 'discrete':\n",
    "        sns.countplot(data=df, x=col, color=\"lightcoral\")\n",
    "        plt.title(f\"Distribution of {col}\", fontsize=11)\n",
    "        plt.xticks(rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f095a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots for continuous numerical features and the target variable (price)\n",
    "continuous_cols = [col for col, var_type in num_cols.items() if var_type == 'continuous']\n",
    "plt.figure(figsize=(16, 10))\n",
    "for i, col in enumerate(continuous_cols, 1):\n",
    "    plt.subplot(3, 2, i)\n",
    "    sns.boxplot(data=df, x=col, color=\"lightblue\")\n",
    "    plt.title(f\"Distribution of {col}\", fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0576a8e8",
   "metadata": {},
   "source": [
    "<a id=\"analysis-of-numerical-distributions\"></a>\n",
    "#### Analysis of Numerical Distributions\n",
    "\n",
    "Key observations from the plots:\n",
    "- **Target Variable (Price)**:\n",
    "  - Highly right-skewed distribution\n",
    "  - Contains significant number of outliers in the upper range\n",
    "  - Most cars are concentrated in the lower price range\n",
    "\n",
    "- **Mileage**:\n",
    "  - Right-skewed distribution\n",
    "  - Large range from nearly new cars to high-mileage vehicles\n",
    "  - Some outliers in upper range suggesting possible data entry errors\n",
    "\n",
    "- **Tax**:\n",
    "  - Multiple peaks suggesting different tax bands\n",
    "  - Contains negative values which require investigation (possible tax benefits/rebates)\n",
    "  - Large number of outliers on both ends of the distribution\n",
    "\n",
    "- **MPG (Miles Per Gallon)**:\n",
    "  - Approximately normal distribution with slight right skew\n",
    "  - Some unrealistic extreme values that need cleaning\n",
    "  - Reasonable median around typical car efficiency ranges\n",
    "\n",
    "- **Paint Quality %**:\n",
    "  - Contains values above 100% which are logically impossible\n",
    "  - Left-skewed distribution suggesting optimistic ratings\n",
    "  - Requires standardization to 0-100 range\n",
    "\n",
    "- **Engine Size**:\n",
    "  - There are engine size with zero values which are not realistic (might indicate electric vehicles)\n",
    "  - Some unusual patterns that need investigation\n",
    "  - Contains outliers that may represent specialty vehicles\n",
    "\n",
    "- **Year**:\n",
    "  - Should be discrete but contains decimal values\n",
    "\n",
    "- **Previous Owners**:\n",
    "  - Should be integer but contains float values\n",
    "  - Right-skewed distribution as expected\n",
    "  - Maximum values need validation (unusually high number of previous owners)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f2c3cb",
   "metadata": {},
   "source": [
    "<a id=\"pre-processing\"></a>\n",
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446f22c3",
   "metadata": {},
   "source": [
    "<a id=\"preprocessing-pipeline-summary\"></a>\n",
    "## Summary of Preprocessing Pipeline\n",
    "\n",
    "The preprocessing is now properly separated:\n",
    "\n",
    "1. **`preprocess_data()`** - Preprocesses a single dataset\n",
    "   - Handles categorical features (standardization, encoding)\n",
    "   - Handles numerical outliers using IQR method\n",
    "   - Imputes missing values with medians\n",
    "   - One-hot encodes categorical features\n",
    "   - Normalizes numerical features with StandardScaler\n",
    "   - Can fit transformers (fit=True) or use existing ones (fit=False)\n",
    "\n",
    "2. **`cross_validate_with_tuning()`** - Performs CV with hyperparameter tuning\n",
    "   - Takes **raw data** (after general_cleaning)\n",
    "   - Applies preprocessing **separately for each fold** (prevents data leakage)\n",
    "   - Performs manual hyperparameter search by sampling from parameter distributions\n",
    "   - Evaluates each combination on validation fold and tracks train/validation performance\n",
    "   - Returns best model configurations\n",
    "\n",
    "3. **`preprocess_test_data()`** - Preprocesses test data\n",
    "   - Uses artifacts from CV to ensure consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e972a0",
   "metadata": {},
   "source": [
    "<a id=\"data-preparation\"></a>\n",
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9194c9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare cleaned data for cross-validation\n",
    "df_cleaned = general_cleaning(df)\n",
    "X = df_cleaned.drop(columns=[\"price\"])\n",
    "y = df_cleaned[\"price\"]\n",
    "\n",
    "# Remove 'price' from num_cols since it's the target\n",
    "del num_cols['price']\n",
    "\n",
    "print(f\"Dataset size: {X.shape}\")\n",
    "print(f\"Target range: £{y.min():.2f} - £{y.max():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cc5ab5",
   "metadata": {},
   "source": [
    "<a id=\"correlation-analysis\"></a>\n",
    "#### Correlation Analysis\n",
    "\n",
    "Before model training, let's examine correlations between numerical features to understand their relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4e09b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for numerical features\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "corr = X[list(num_cols.keys())].corr(method=\"pearson\")\n",
    "sns.heatmap(data=corr, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Matrix of Numerical Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f115e4",
   "metadata": {},
   "source": [
    "<a id=\"model-training\"></a>\n",
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfc3b63",
   "metadata": {},
   "source": [
    "<a id=\"model-selection-with-cv\"></a>\n",
    "### Model Selection with Cross-Validation\n",
    "\n",
    "We'll use cross-validation with hyperparameter tuning to select the best model. Configure your model using a dictionary with the model class, parameter distributions, and number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e1e089",
   "metadata": {},
   "source": [
    "<a id=\"quick-baseline-model\"></a>\n",
    "### Quick Baseline Model\n",
    "\n",
    "Before running extensive CV, let's train a simple baseline model for quick reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a55e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick train/val split for baseline\n",
    "X_train_baseline, X_val_baseline, y_train_baseline, y_val_baseline = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=SEED\n",
    ")\n",
    "\n",
    "# Preprocess baseline data\n",
    "X_train_processed, baseline_artifacts = preprocess_data(X_train_baseline, cat_cols, num_cols, fit=True)\n",
    "X_val_processed = preprocess_data(X_val_baseline, cat_cols, num_cols, artifacts=baseline_artifacts, fit=False)\n",
    "\n",
    "# Train simple Ridge model\n",
    "baseline_model = Ridge(alpha=1.0, fit_intercept=True)\n",
    "\n",
    "if LOG_TARGET:\n",
    "    baseline_model.fit(X_train_processed, np.log1p(y_train_baseline))\n",
    "    y_train_pred = np.expm1(baseline_model.predict(X_train_processed))\n",
    "    y_val_pred = np.expm1(baseline_model.predict(X_val_processed))\n",
    "else:\n",
    "    baseline_model.fit(X_train_processed, y_train_baseline)\n",
    "    y_train_pred = baseline_model.predict(X_train_processed)\n",
    "    y_val_pred = baseline_model.predict(X_val_processed)\n",
    "\n",
    "mae_train = mean_absolute_error(y_train_baseline, y_train_pred)\n",
    "mae_val = mean_absolute_error(y_val_baseline, y_val_pred)\n",
    "r2_val = r2_score(y_val_baseline, y_val_pred)\n",
    "\n",
    "print(f\"Baseline Ridge (alpha=1.0):\")\n",
    "print(f\"  Train MAE: £{mae_train:.2f}\")\n",
    "print(f\"  Val MAE:   £{mae_val:.2f}\")\n",
    "print(f\"  Val R²:    {r2_val:.4f}\")\n",
    "print(f\"\\nThis gives us a reference point before hyperparameter tuning with CV.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5747bc65",
   "metadata": {},
   "source": [
    "<a id=\"feature-importance-analysis\"></a>\n",
    "### Feature Importance Analysis\n",
    "\n",
    "In this section, we will train 4 different models (Ridge, Random Forest, Gradient Boosting, and Extra Trees) to analyze feature importance. Based on the results, we will select a subset of features to use for the final hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef36111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models for Feature Importance Analysis\n",
    "# We use default parameters or a small search space for quick analysis\n",
    "\n",
    "# 1. Ridge (Linear)\n",
    "print(\"--- Ridge Feature Importance ---\")\n",
    "fi_ridge_config = {\n",
    "    'model_class': Ridge, \n",
    "    'param_distributions': {'alpha': [1.0]}, \n",
    "    'n_iter': 1\n",
    "}\n",
    "fi_ridge_results = cross_validate_with_tuning(\n",
    "    X, y, cat_cols, num_cols, fi_ridge_config, k=3, seed=SEED, verbose=False\n",
    ")\n",
    "\n",
    "# 2. Random Forest\n",
    "print(\"\\n--- Random Forest Feature Importance ---\")\n",
    "fi_rf_config = {\n",
    "    'model_class': RandomForestRegressor, \n",
    "    'param_distributions': {'n_estimators': [100], 'max_depth': [10]}, \n",
    "    'n_iter': 1\n",
    "}\n",
    "fi_rf_results = cross_validate_with_tuning(\n",
    "    X, y, cat_cols, num_cols, fi_rf_config, k=3, seed=SEED, verbose=False\n",
    ")\n",
    "\n",
    "# 3. Gradient Boosting\n",
    "print(\"\\n--- Gradient Boosting Feature Importance ---\")\n",
    "fi_gb_config = {\n",
    "    'model_class': GradientBoostingRegressor, \n",
    "    'param_distributions': {'n_estimators': [100], 'max_depth': [5]}, \n",
    "    'n_iter': 1\n",
    "}\n",
    "fi_gb_results = cross_validate_with_tuning(\n",
    "    X, y, cat_cols, num_cols, fi_gb_config, k=3, seed=SEED, verbose=False\n",
    ")\n",
    "\n",
    "# 4. Extra Trees\n",
    "print(\"\\n--- Extra Trees Feature Importance ---\")\n",
    "fi_et_config = {\n",
    "    'model_class': ExtraTreesRegressor, \n",
    "    'param_distributions': {'n_estimators': [100], 'max_depth': [10]}, \n",
    "    'n_iter': 1\n",
    "}\n",
    "fi_et_results = cross_validate_with_tuning(\n",
    "    X, y, cat_cols, num_cols, fi_et_config, k=3, seed=SEED, verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a40f8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Feature Importance for Ridge\n",
    "get_feature_importance(fi_ridge_results['best_estimator'], \n",
    "                       preprocess_data(X, cat_cols, num_cols, artifacts=fi_ridge_results['final_artifacts'], fit=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdcbfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Feature Importance for Random Forest\n",
    "get_feature_importance(fi_rf_results['best_estimator'], \n",
    "                       preprocess_data(X, cat_cols, num_cols, artifacts=fi_rf_results['final_artifacts'], fit=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b3c523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Feature Importance for Gradient Boosting\n",
    "get_feature_importance(fi_gb_results['best_estimator'], \n",
    "                       preprocess_data(X, cat_cols, num_cols, artifacts=fi_gb_results['final_artifacts'], fit=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a2c921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Feature Importance for Extra Trees\n",
    "get_feature_importance(fi_et_results['best_estimator'], \n",
    "                       preprocess_data(X, cat_cols, num_cols, artifacts=fi_et_results['final_artifacts'], fit=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c0237f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of all processed feature names from one of the artifacts\n",
    "all_features = list(preprocess_data(X, cat_cols, num_cols, artifacts=fi_ridge_results['final_artifacts'], fit=False).columns)\n",
    "\n",
    "print(\"Original Feature Count:\", len(all_features))\n",
    "\n",
    "# Collect feature importance from all models\n",
    "feature_importance_df = pd.DataFrame(index=all_features)\n",
    "\n",
    "models_fi = {\n",
    "    'Ridge': fi_ridge_results,\n",
    "    'Random Forest': fi_rf_results,\n",
    "    'Gradient Boosting': fi_gb_results,\n",
    "    'Extra Trees': fi_et_results\n",
    "}\n",
    "\n",
    "for name, result in models_fi.items():\n",
    "    # Reconstruct processed X to match columns\n",
    "    X_processed = preprocess_data(X, cat_cols, num_cols, artifacts=result['final_artifacts'], fit=False)\n",
    "    \n",
    "    # Get importance dataframe (plot=False to avoid duplicate plots)\n",
    "    imp_df = get_feature_importance(result['best_estimator'], X_processed, plot=False)\n",
    "    \n",
    "    # Map values to the main dataframe\n",
    "    # imp_df has columns: Feature, Value, Method\n",
    "    # We set the index to Feature and extract Value\n",
    "    imp_series = imp_df.set_index('Feature')['Value']\n",
    "    feature_importance_df[name] = imp_series\n",
    "\n",
    "# Normalize each column to 0-1 range\n",
    "scaler = MinMaxScaler()\n",
    "feature_importance_normalized = pd.DataFrame(\n",
    "    scaler.fit_transform(feature_importance_df.fillna(0)), \n",
    "    columns=feature_importance_df.columns, \n",
    "    index=feature_importance_df.index\n",
    ")\n",
    "\n",
    "# Calculate mean importance\n",
    "feature_importance_normalized['Mean_Importance'] = feature_importance_normalized.mean(axis=1)\n",
    "feature_importance_normalized = feature_importance_normalized.sort_values('Mean_Importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 15 Features by Mean Importance:\")\n",
    "print(feature_importance_normalized['Mean_Importance'].head(15))\n",
    "\n",
    "# Selection Strategy: Keep features with mean importance > 0.005\n",
    "threshold = 0.05\n",
    "selected_features = feature_importance_normalized[feature_importance_normalized['Mean_Importance'] > threshold].index.tolist()\n",
    "\n",
    "print(f\"\\nSelected {len(selected_features)} features (Mean Importance > {threshold}).\")\n",
    "print(f\"Dropped {len(all_features) - len(selected_features)} features: {list(set(all_features) - set(selected_features))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb601b4",
   "metadata": {},
   "source": [
    "<a id=\"experiment-algorithms\"></a>\n",
    "### Experiment Algorithms\n",
    "\n",
    "Now we'll experiment with different algorithms using cross-validation with hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab6b358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Ridge Regression with hyperparameter tuning\n",
    "ridge_config = {\n",
    "    'model_class': Ridge,\n",
    "    'param_distributions': {\n",
    "        'alpha': loguniform(1e-3, 1e2),\n",
    "        'fit_intercept': [True, False]\n",
    "    },\n",
    "    'n_iter': 20\n",
    "}\n",
    "\n",
    "ridge_results = cross_validate_with_tuning(\n",
    "    X, \n",
    "    y, \n",
    "    cat_cols, \n",
    "    num_cols, \n",
    "    ridge_config, \n",
    "    k=3, \n",
    "    seed=SEED, \n",
    "    selected_features=selected_features, \n",
    "    log_target=LOG_TARGET\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ebdfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Lasso Regression\n",
    "lasso_config = {\n",
    "    'model_class': Lasso,\n",
    "    'param_distributions': {\n",
    "        'alpha': loguniform(1e-3, 1e2),\n",
    "        'fit_intercept': [True, False]\n",
    "    },\n",
    "    'n_iter': 20\n",
    "}\n",
    "\n",
    "lasso_results = cross_validate_with_tuning(\n",
    "    X, \n",
    "    y, \n",
    "    cat_cols, \n",
    "    num_cols, \n",
    "    lasso_config, \n",
    "    k=3, \n",
    "    seed=SEED, \n",
    "    selected_features=selected_features, \n",
    "    log_target=LOG_TARGET\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8fb232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Random Forest\n",
    "rf_config = {\n",
    "    'model_class': RandomForestRegressor,\n",
    "    'param_distributions': {\n",
    "        'n_estimators': randint(50, 200),\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': randint(2, 20),\n",
    "        'min_samples_leaf': randint(1, 10),\n",
    "        'max_features': ['sqrt', 0.5],\n",
    "        'random_state': SEED\n",
    "    },\n",
    "    'n_iter': 10\n",
    "}\n",
    "\n",
    "rf_results = cross_validate_with_tuning(\n",
    "    X, \n",
    "    y, \n",
    "    cat_cols, \n",
    "    num_cols, \n",
    "    rf_config, \n",
    "    k=3, \n",
    "    seed=SEED, \n",
    "    selected_features=selected_features, \n",
    "    log_target=LOG_TARGET\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45536493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Hist Gradient Boosting Regressor\n",
    "hgb_config = {\n",
    "    'model_class': HistGradientBoostingRegressor,\n",
    "    'param_distributions': {\n",
    "        'learning_rate': loguniform(0.01, 0.3),\n",
    "        'max_depth': randint(3, 10),\n",
    "        'max_leaf_nodes': randint(15, 63),\n",
    "        'l2_regularization': loguniform(1e-3, 10),\n",
    "        'max_iter': [100, 200],\n",
    "        'loss': ['absolute_error'],\n",
    "        'random_state': SEED\n",
    "    },\n",
    "    'n_iter': 20\n",
    "}\n",
    "\n",
    "hgb_results = cross_validate_with_tuning(\n",
    "    X, \n",
    "    y, \n",
    "    cat_cols, \n",
    "    num_cols, \n",
    "    hgb_config, \n",
    "    k=3, \n",
    "    seed=SEED, \n",
    "    selected_features=selected_features, \n",
    "    log_target=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1256929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5: Gradient Boosting Regressor (Standard)\n",
    "gb_config = {\n",
    "    'model_class': GradientBoostingRegressor,\n",
    "    'param_distributions': {\n",
    "        'n_estimators': randint(50, 200),\n",
    "        'learning_rate': loguniform(0.01, 0.2),\n",
    "        'max_depth': randint(3, 6),\n",
    "        'min_samples_split': randint(2, 20),\n",
    "        'min_samples_leaf': randint(1, 10),\n",
    "        'subsample': [0.8, 0.9, 1.0],\n",
    "        'random_state': SEED\n",
    "    },\n",
    "    'n_iter': 10\n",
    "}\n",
    "\n",
    "gb_results = cross_validate_with_tuning(\n",
    "    X, \n",
    "    y, \n",
    "    cat_cols, \n",
    "    num_cols, \n",
    "    gb_config, \n",
    "    k=3, \n",
    "    seed=SEED, \n",
    "    selected_features=selected_features, \n",
    "    log_target=LOG_TARGET\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4740b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 6: Extra Trees Regressor\n",
    "et_config = {\n",
    "    'model_class': ExtraTreesRegressor,\n",
    "    'param_distributions': {\n",
    "        'n_estimators': randint(50, 200),\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': randint(2, 20),\n",
    "        'min_samples_leaf': randint(1, 10),\n",
    "        'max_features': ['sqrt', 0.5],\n",
    "        'random_state': SEED\n",
    "    },\n",
    "    'n_iter': 15\n",
    "}\n",
    "\n",
    "et_results = cross_validate_with_tuning(\n",
    "    X, \n",
    "    y, \n",
    "    cat_cols, \n",
    "    num_cols, \n",
    "    et_config, \n",
    "    k=3, \n",
    "    seed=SEED, \n",
    "    selected_features=selected_features, \n",
    "    log_target=LOG_TARGET\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830432df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 7: MLP Regressor (Neural Network)\n",
    "mlp_config = {\n",
    "    'model_class': MLPRegressor,\n",
    "    'param_distributions': {\n",
    "        'hidden_layer_sizes': [(32,), (64,), (100,), (32, 16), (64, 32), (100, 50)],\n",
    "        'activation': ['relu', 'tanh'],\n",
    "        'solver': 'adam',\n",
    "        'alpha': loguniform(1e-5, 1e-1),\n",
    "        'learning_rate_init': loguniform(1e-4, 1e-2),\n",
    "        'max_iter': 500,\n",
    "        'early_stopping': True,\n",
    "        'random_state': SEED\n",
    "    },\n",
    "    'n_iter': 10\n",
    "}\n",
    "\n",
    "mlp_results = cross_validate_with_tuning(\n",
    "    X, \n",
    "    y, \n",
    "    cat_cols, \n",
    "    num_cols, \n",
    "    mlp_config, \n",
    "    k=3, \n",
    "    seed=SEED, \n",
    "    selected_features=selected_features, \n",
    "    log_target=LOG_TARGET\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d70f1b",
   "metadata": {},
   "source": [
    "### Final Model Selection\n",
    "\n",
    "I will select the best algorithm with the best hyperparameters and train the model with it. While doing it I want to use all available data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652a272f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models to find the best one\n",
    "results = {\n",
    "    'Ridge': ridge_results,\n",
    "    'Lasso': lasso_results,\n",
    "    'Random Forest': rf_results,\n",
    "    'Hist Gradient Boosting': hgb_results,\n",
    "    'Gradient Boosting': gb_results,\n",
    "    'Extra Trees': et_results,\n",
    "    'MLP': mlp_results\n",
    "}\n",
    "\n",
    "# Select the best model based on the lowest mean CV score (MAE)\n",
    "best_model_name = min(results, key=lambda k: results[k]['mean_cv_score'])\n",
    "best_result = results[best_model_name]\n",
    "\n",
    "print(f\"Best Model Selected: {best_model_name}\")\n",
    "print(f\"Best CV MAE: £{best_result['mean_cv_score']:.2f} ± £{best_result['std_cv_score']:.2f}\")\n",
    "print(f\"Best Parameters: {best_result['best_params']}\\n\")\n",
    "\n",
    "# The function now returns the model already fitted on ALL available data\n",
    "# and the corresponding preprocessing artifacts\n",
    "best_model = best_result['best_estimator']\n",
    "final_artifacts = best_result['final_artifacts']\n",
    "\n",
    "print(f\"Final model ({best_model_name}) is ready and fitted on all data.\")\n",
    "\n",
    "# Visualize Feature Importance (if applicable)\n",
    "try:\n",
    "    # We need the processed feature names for the plot\n",
    "    X_all_processed = preprocess_data(X, cat_cols, num_cols, artifacts=final_artifacts, fit=False)\n",
    "    \n",
    "    # Filter selected features if they were used during training\n",
    "    if 'selected_features' in final_artifacts:\n",
    "        X_all_processed = X_all_processed[final_artifacts['selected_features']]\n",
    "        \n",
    "    get_feature_importance(best_model, X_all_processed, model_class=type(best_model))\n",
    "except Exception as e:\n",
    "    print(f\"Could not plot feature importance: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb360862",
   "metadata": {},
   "source": [
    "<a id=\"predictions\"></a>\n",
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb33e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess test data\n",
    "test_df = pd.read_csv('data/test.csv').set_index('carID')\n",
    "\n",
    "# Use the artifacts from the final fit on all data\n",
    "test_processed = preprocess_test_data(test_df, final_artifacts)\n",
    "\n",
    "# Make predictions\n",
    "if final_artifacts.get('log_target', True):\n",
    "    test_predictions = np.expm1(best_model.predict(test_processed))\n",
    "else:\n",
    "    test_predictions = best_model.predict(test_processed)\n",
    "\n",
    "# Save predictions\n",
    "predictions_df = pd.DataFrame({'price': test_predictions}, index=test_df.index)\n",
    "predictions_df.to_csv('data/test_predictions.csv')\n",
    "\n",
    "print(f\"Predictions saved for {len(test_predictions)} test samples\")\n",
    "print(f\"Predicted price range: £{test_predictions.min():.2f} - £{test_predictions.max():.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataMining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
