{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "891cffd7",
   "metadata": {},
   "source": [
    "# Machine Learning Project\n",
    "\n",
    "## Table of Contents\n",
    "- [Import Data](#import-data)\n",
    "  - [Import Data Summary](#import-data-summary)\n",
    "- [Data Exploration](#data-exploration)\n",
    "  - [Boolean Features](#boolean-features)\n",
    "    - [Boolean Features Analysis](#boolean-features-analysis)\n",
    "  - [Categorical Features](#categorical-features)\n",
    "    - [Check Categorical Features Consistency](#check-categorical-features-consistency)\n",
    "    - [Categorical Features Summary](#categorical-features-summary)\n",
    "  - [Numerical Features](#numerical-features)\n",
    "    - [Numerical Plots](#plots)\n",
    "    - [Analysis of Numerical Distributions](#analysis-of-numerical-distributions)\n",
    "- [Pre-processing](#pre-processing)\n",
    "  - [Functions](#functions)\n",
    "  - [Split Data](#split-data)\n",
    "  - [Outliers and Missing Values](#outliers-and-missing-values)\n",
    "    - [Categorical Outliers and Missing Values](#categorical-outliers-and-missing-values)\n",
    "    - [Numerical Outliers](#numerical-outliers)\n",
    "    - [Numerical Missing Values](#numerical-missing-values)\n",
    "- [Feature Engineering](#feature-engineering)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47693163",
   "metadata": {},
   "source": [
    "<a id=\"import-data\"></a>\n",
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9c6c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# to calculate distance between strings\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6468f612",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv').set_index('carID')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc133ea3-1850-4386-ab5e-6db9950c48f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a5a91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_duplicated_ids = df.index.duplicated().sum()\n",
    "print(f'Number of duplicated carIDs: {num_duplicated_ids}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c607395",
   "metadata": {},
   "source": [
    "<a id=\"import-data-summary\"></a>\n",
    "#### Import Data Summary\n",
    "- Dataset loaded successfully with `carID` as the index\n",
    "- There are no duplicate entries in carID\n",
    "- The dataset contains information about cars including both numerical features (price, mileage, tax, etc.) and categorical features (brand, model, transmission, etc.)\n",
    "- Initial inspection shows multiple features that will require preprocessing:\n",
    "  - Numerical features that need cleaning (negative values, outliers)\n",
    "  - Categorical features that need standardization\n",
    "  - Presence of missing values in several columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3db49c",
   "metadata": {},
   "source": [
    "<a id=\"data-exploration\"></a>\n",
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb63f270",
   "metadata": {},
   "source": [
    "<a id=\"boolean-features\"></a>\n",
    "### Boolean Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e5927d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hasDamage'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb51349",
   "metadata": {},
   "source": [
    "<a id=\"boolean-features-analysis\"></a>\n",
    "#### Boolean Features Analysis\n",
    "\n",
    "Key observations about `hasDamage` feature:\n",
    "- Only contains binary values (0) and NaN\n",
    "- No instances of value 1 found, suggesting potential data collection issues\n",
    "- May indicate:\n",
    "  - Cars with damage not being listed\n",
    "  - System default setting of 0 for non-damaged cars\n",
    "  - Incomplete damage assessment process\n",
    "- Requires special handling in preprocessing:\n",
    "  - Consider treating NaN as a separate category\n",
    "  - Validate if 0 truly represents \"no damage\"\n",
    "  - May need to be treated as a categorical rather than boolean feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0273ca",
   "metadata": {},
   "source": [
    "<a id=\"categorical-features\"></a>\n",
    "### Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4432c021",
   "metadata": {},
   "source": [
    "<a id=\"check-categorical-features-consistency\"></a>\n",
    "#### Check Categorical Features Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095f1162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of categorical features\n",
    "cat_cols = ['Brand', 'model', 'fuelType', 'transmission']\n",
    "\n",
    "# Identify outlier examples in categorical features\n",
    "cat_outliers_examples = {col: df[col].value_counts().tail(10).index for col in cat_cols}\n",
    "\n",
    "# Display the outlier examples\n",
    "pd.DataFrame(cat_outliers_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa457064",
   "metadata": {},
   "source": [
    "<a id=\"categorical-features-summary\"></a>\n",
    "#### Categorical Features Summary\n",
    "- Initial analysis reveals significant data quality issues across all categorical columns\n",
    "- No standardization in categorical features, with multiple variations of the same values (different spellings, capitalizations)\n",
    "- Solution: We will implement string distance-based standardization using the `thefuzz` library to clean and standardize these features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ab1e0a",
   "metadata": {},
   "source": [
    "<a id=\"numerical-features\"></a>\n",
    "### Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6a26e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict of numerical features\n",
    "num_cols = {\n",
    "    'price': 'continuous',\n",
    "    'mileage': 'continuous',\n",
    "    'tax': 'continuous',\n",
    "    'mpg': 'continuous',\n",
    "    'paintQuality%': 'continuous',\n",
    "    'engineSize': 'continuous',\n",
    "    'year': 'discrete',\n",
    "    'previousOwners': 'discrete'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15d6909",
   "metadata": {},
   "source": [
    "<a id=\"plots\"></a>\n",
    "#### Numerical Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d22957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot figures for numerical features and the target variable (price)\n",
    "plt.figure(figsize=(16, 10))\n",
    "for i, (col, var_type) in enumerate(num_cols.items(), 1):\n",
    "    plt.subplot(4, 2, i)\n",
    "\n",
    "    # Plot based on variable type\n",
    "    if var_type == 'continuous':\n",
    "        sns.histplot(data=df, x=col, kde=True, color=\"lightcoral\", bins=30)\n",
    "        plt.title(f\"Distribution of {col}\", fontsize=11)\n",
    "    elif var_type == 'discrete':\n",
    "        sns.countplot(data=df, x=col, color=\"lightcoral\")\n",
    "        plt.title(f\"Distribution of {col}\", fontsize=11)\n",
    "        plt.xticks(rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f095a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots for continuous numerical features and the target variable (price)\n",
    "continuous_cols = [col for col, var_type in num_cols.items() if var_type == 'continuous']\n",
    "plt.figure(figsize=(16, 10))\n",
    "for i, col in enumerate(continuous_cols, 1):\n",
    "    plt.subplot(3, 2, i)\n",
    "    sns.boxplot(data=df, x=col, color=\"lightblue\")\n",
    "    plt.title(f\"Distribution of {col}\", fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0576a8e8",
   "metadata": {},
   "source": [
    "<a id=\"analysis-of-numerical-distributions\"></a>\n",
    "#### Analysis of Numerical Distributions\n",
    "\n",
    "Key observations from the plots:\n",
    "- **Target Variable (Price)**:\n",
    "  - Highly right-skewed distribution\n",
    "  - Contains significant number of outliers in the upper range\n",
    "  - Most cars are concentrated in the lower price range\n",
    "\n",
    "- **Mileage**:\n",
    "  - Right-skewed distribution\n",
    "  - Large range from nearly new cars to high-mileage vehicles\n",
    "  - Some outliers in upper range suggesting possible data entry errors\n",
    "\n",
    "- **Tax**:\n",
    "  - Multiple peaks suggesting different tax bands\n",
    "  - Contains negative values which require investigation (possible tax benefits/rebates)\n",
    "  - Large number of outliers on both ends of the distribution\n",
    "\n",
    "- **MPG (Miles Per Gallon)**:\n",
    "  - Approximately normal distribution with slight right skew\n",
    "  - Some unrealistic extreme values that need cleaning\n",
    "  - Reasonable median around typical car efficiency ranges\n",
    "\n",
    "- **Paint Quality %**:\n",
    "  - Contains values above 100% which are logically impossible\n",
    "  - Left-skewed distribution suggesting optimistic ratings\n",
    "  - Requires standardization to 0-100 range\n",
    "\n",
    "- **Engine Size**:\n",
    "  - There are engine size with zero values which are not realistic (might indicate electric vehicles)\n",
    "  - Some unusual patterns that need investigation\n",
    "  - Contains outliers that may represent specialty vehicles\n",
    "\n",
    "- **Year**:\n",
    "  - Should be discrete but contains decimal values\n",
    "\n",
    "- **Previous Owners**:\n",
    "  - Should be integer but contains float values\n",
    "  - Right-skewed distribution as expected\n",
    "  - Maximum values need validation (unusually high number of previous owners)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f2c3cb",
   "metadata": {},
   "source": [
    "<a id=\"pre-processing\"></a>\n",
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0225083",
   "metadata": {},
   "source": [
    "<a id=\"functions\"></a>\n",
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6c0933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  general_cleaning(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Perform general data cleaning on the DataFrame.\n",
    "    \n",
    "    This function handles logical inconsistencies and data quality issues that\n",
    "    don't require statistical calculations (mean, median, etc.) to prevent data\n",
    "    leakage between training and validation sets.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame containing car data with columns:\n",
    "            Brand, model, year, transmission, fuelType, mileage, tax, mpg, \n",
    "            engineSize, paintQuality%, previousOwners, hasDamage.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: The cleaned DataFrame with logical inconsistencies resolved.\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # Set negative values to NaN for features that shouldn't be negative\n",
    "    for col in ['previousOwners', 'mileage', 'tax', 'mpg', 'engineSize']:\n",
    "        df.loc[df[col] < 0, col] = np.nan\n",
    "\n",
    "    for col in ['Brand', 'model', 'transmission', 'fuelType']:\n",
    "        df[col] = df[col].str.lower()\n",
    "        df[col] = df[col].replace('', np.nan)\n",
    "\n",
    "    # Remove decimal part from 'year'\n",
    "    df['year'] = np.floor(df['year']).astype('Int64')\n",
    "\n",
    "    # Remove decimal part from 'previousOwners'\n",
    "    df['previousOwners'] = np.floor(df['previousOwners']).astype('Int64')\n",
    "\n",
    "    # Ensure 'paintQuality%' is within 0-100\n",
    "    df.loc[(df['paintQuality%'] < 0) | (df['paintQuality%'] > 100), 'paintQuality%'] = np.nan\n",
    "\n",
    "    # Fill missing 'hasDamage' with 1\n",
    "    df['hasDamage'] = df['hasDamage'].fillna(1).astype('Int64')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48fcfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_categorical_col(series: pd.Series, \n",
    "                                standardised_cats: list[str], \n",
    "                                distance_threshold: int = 2) -> pd.Series:\n",
    "    \"\"\"Standardizes a categorical column using edit distance with a threshold.\n",
    "\n",
    "    1. Maps values to a standard category if they are a likely typo\n",
    "       (i.e., within the edit distance_threshold).\n",
    "    2. Keeps values that are already in the standard list.\n",
    "    3. Groups all other values that don't match into an 'other' bin.\n",
    "    \n",
    "    Args:\n",
    "        series (pd.Series): The categorical column to standardize.\n",
    "        standardised_cats (list[str]): The list of \"good\" categories to match against.\n",
    "        distance_threshold (int): The max edit distance to consider something a typo.\n",
    "                                  A value of 1 or 2 is recommended.\n",
    "                                  \n",
    "    Returns:\n",
    "        pd.Series: The standardized categorical column.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Get all unique non-null values from the series\n",
    "    unique_values = series.dropna().unique()\n",
    "    \n",
    "    # 2. Build the mapping dictionary\n",
    "    mapping = {}\n",
    "    \n",
    "    for x in unique_values:\n",
    "        x_str = str(x)\n",
    "        \n",
    "        # Check if it's already a perfect match\n",
    "        if x_str in standardised_cats:\n",
    "            mapping[x] = x_str\n",
    "            continue\n",
    "\n",
    "        # Find the closest match and its distance\n",
    "        distances = [nltk.edit_distance(x_str, cat) for cat in standardised_cats]\n",
    "        min_distance = np.min(distances)\n",
    "        \n",
    "        if min_distance <= distance_threshold:\n",
    "            closest_cat = standardised_cats[np.argmin(distances)]\n",
    "            mapping[x] = closest_cat\n",
    "        else:\n",
    "            mapping[x] = 'other'\n",
    "            \n",
    "    return series.map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9309596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_high_freq(series: pd.Series, percent_threshold: float = 0.02) -> list[str]:\n",
    "    \"\"\"Get categories that appear more than a dynamic percentage threshold.\n",
    "    \n",
    "    Args:\n",
    "        series (pd.Series): The categorical series to analyze.\n",
    "        percent_threshold (float): The minimum percentage of total rows a category\n",
    "                                   must have to be included (e.g., 0.01 for 1%).\n",
    "                                   \n",
    "    Returns:\n",
    "        list[str]: List of categories with frequency above the dynamic threshold.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the dynamic count threshold based on the percentage\n",
    "    dynamic_count_threshold = len(series) * percent_threshold\n",
    "    \n",
    "    value_counts = series.value_counts()\n",
    "    \n",
    "    # Use the *same logic* as before, but with the new dynamic threshold\n",
    "    high_freq_cats = value_counts[value_counts > dynamic_count_threshold].index.tolist()\n",
    "    \n",
    "    return high_freq_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1795c746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_upper_bound(series: pd.Series) -> float:\n",
    "    \"\"\"Calculates the upper outlier bound for a pandas Series.\"\"\"\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    return Q3 + (1.5 * IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3254edf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_outliers(series: pd.Series, \n",
    "                   upper_bound: float,\n",
    "                   lower_bound: float = 0.0, \n",
    "                   return_missing: bool = True) -> pd.Series:\n",
    "    \"\"\"Clean outliers in the Series based on specified bounds.\n",
    "\n",
    "    This function clips values outside the specified bounds or sets them to NaN.\n",
    "    \n",
    "    Args:\n",
    "        series (pd.Series): The input Series containing numerical data.\n",
    "        lower_bound (float): The lower bound for valid values.\n",
    "        upper_bound (float): The upper bound for valid values.\n",
    "        return_missing (bool): If True, set out-of-bound values to NaN.\n",
    "                              If False, clip values to the bounds.\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series: The cleaned Series with outliers handled.\n",
    "    \"\"\"\n",
    "    cleaned = series.copy()\n",
    "    \n",
    "    if return_missing:\n",
    "        # Set out-of-bound values to NaN\n",
    "        cleaned[(cleaned < lower_bound) | (cleaned > upper_bound)] = np.nan\n",
    "    else:\n",
    "        # Clip values to the specified bounds\n",
    "        cleaned = cleaned.clip(lower=lower_bound, upper=upper_bound)\n",
    "    \n",
    "    return cleaned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e972a0",
   "metadata": {},
   "source": [
    "<a id=\"split-data\"></a>\n",
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9194c9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply general cleaning to the entire dataset, before splitting\n",
    "df_cleaned = general_cleaning(df)\n",
    "\n",
    "# Split the cleaned data into training and validation sets\n",
    "X = df_cleaned.drop(columns=[\"price\"])   \n",
    "y = df_cleaned[\"price\"]\n",
    "del num_cols['price']\n",
    "continuous_cols.remove('price')\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Validation set size: {X_val.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce60cb3",
   "metadata": {},
   "source": [
    "<a id=\"outliers-and-missing-values\"></a>\n",
    "### Outliers and Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979b2a8e",
   "metadata": {},
   "source": [
    "<a id=\"categorical-outliers-and-missing-values\"></a>\n",
    "#### Categorical Outliers and Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b22f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map of categorical columns to their high frequency categories\n",
    "# Using only the categories in train\n",
    "high_freq_categories_by_col = {col: get_categories_high_freq(X_train[col]) for col in cat_cols}\n",
    "\n",
    "for col in cat_cols:\n",
    "    standardised_cats = high_freq_categories_by_col[col]\n",
    "    # process training data\n",
    "    X_train[col] = standardize_categorical_col(X_train[col], standardised_cats)\n",
    "    X_train[col] =X_train[col].fillna('other')\n",
    "    \n",
    "    # process validation data\n",
    "    X_val[col] = standardize_categorical_col(X_val[col], standardised_cats)\n",
    "    X_val[col] = X_val[col].fillna('other')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ce953d",
   "metadata": {},
   "source": [
    "<a id=\"numerical-outliers\"></a>\n",
    "#### Numerical Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5010863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mileage is right_skewed and extremes might exist\n",
    "mileage_upper_bound = X_train['mileage'].quantile(0.95)\n",
    "\n",
    "# Calculate upper bounds for each numerical features\n",
    "upper_bounds_num_cols = {col: calculate_upper_bound(X_train[col]) for col in continuous_cols}\n",
    "\n",
    "# Winsorization at 95th percentile\n",
    "X_train['mileage'] = clean_outliers(X_train['mileage'], \n",
    "                                     lower_bound=0, \n",
    "                                     upper_bound=mileage_upper_bound, \n",
    "                                     return_missing=False)\n",
    "\n",
    "\n",
    "# Turn outliers for remaining columns into np.nan\n",
    "for col in continuous_cols:\n",
    "    if col == 'mileage':\n",
    "        continue\n",
    "    upper_bound = upper_bounds_num_cols[col]\n",
    "    \n",
    "    X_train[col] = clean_outliers(X_train[col], upper_bound)\n",
    "    X_val[col] = clean_outliers(X_val[col], upper_bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacc0fcf",
   "metadata": {},
   "source": [
    "<a id=\"numerical-missing-values\"></a>\n",
    "#### Numerical Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bb8f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_num_cols = {col: X_train[col].median() for col in num_cols}\n",
    "\n",
    "for col in num_cols:\n",
    "    calc_median = median_num_cols[col]\n",
    "    X_train[col] = X_train[col].fillna(calc_median)\n",
    "    X_val[col] = X_val[col].fillna(calc_median)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f09a049",
   "metadata": {},
   "source": [
    "<a id=\"feature-engineering\"></a>\n",
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0b86ef",
   "metadata": {},
   "source": [
    "Transform categorical variables into numeric ones using One-Hot Encoding.\n",
    "One-Hot Encoding creates a separate binary column for each category, avoiding any unintended ordinal relationship among categorical values and allowing the model to interpret each category independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e2b379",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = cat_cols\n",
    "numeric_features = list(num_cols.keys()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9974dd1e",
   "metadata": {},
   "source": [
    "The parameter handle_unknown=\"ignore\" ensures that unseen categories in the validation set do not cause errors during transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824f3295",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "ohe_train = pd.DataFrame(\n",
    "    ohe.fit_transform(X_train[categorical_features]),\n",
    "    columns=ohe.get_feature_names_out(categorical_features),\n",
    "    index=X_train.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff40407f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_val = pd.DataFrame(\n",
    "    ohe.transform(X_val[categorical_features]),\n",
    "    columns=ohe.get_feature_names_out(categorical_features),\n",
    "    index=X_val.index\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5aa6c89",
   "metadata": {},
   "source": [
    "The numeric variables were then standardized using Z-Score normalization through StandardScaler.\n",
    "This process ensures that all numeric features have a mean of 0 and a standard deviation of 1, keeping them on a comparable scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6447bc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([X_train.drop(columns=categorical_features), ohe_train], axis=1)\n",
    "X_val   = pd.concat([X_val.drop(columns=categorical_features),   ohe_val],   axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d6d302",
   "metadata": {},
   "source": [
    "### Correlation Matrix \n",
    "In order to help with both feature selection and engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35af957d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "\n",
    "corr = X_train[list(num_cols.keys())].corr(method=\"pearson\")\n",
    "\n",
    "sns.heatmap(data=corr, annot=True, )\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456e15c9",
   "metadata": {},
   "source": [
    "## Observations\n",
    "- Most correlations are weak, meaning variables are largely independent\n",
    "\n",
    "---\n",
    "\n",
    "## Insights\n",
    "\n",
    "### **Mileage vs Year (-0.68)**\n",
    "- **Strong negative correlation.**\n",
    "- Newer cars (higher `year`) tend to have lower mileage.\n",
    "- These two features describe similar aspects of car age/usage.\n",
    "-  **Potential issue:** Including both in models may cause multicollinearity.\n",
    "\n",
    "---\n",
    "\n",
    "###  **Tax vs mpg (-0.51)**\n",
    "- **Moderate negative correlation.**\n",
    "- Cars with better fuel efficiency (higher mpg) tend to have lower taxes, likely due to emissions-based tax systems.\n",
    "\n",
    "---\n",
    "\n",
    "### **Tax vs Year (0.35)**\n",
    "- **Weak positive correlation.**\n",
    "- Newer cars may have slightly higher taxes, possibly reflecting newer model valuations or updated emission standards.\n",
    "\n",
    "---\n",
    "\n",
    "### **Mileage vs mpg (0.25)**\n",
    "- **Weak positive correlation.**\n",
    "- Slightly counterintuitive — could suggest that cars with better mpg are driven more (used as daily vehicles).\n",
    "\n",
    "---\n",
    "\n",
    "###  **Other variables**\n",
    "- `paintQuality%`, `engineSize`, and `previousOwners` show very low correlations (~ 0) with other variables.  \n",
    "- These likely capture unique information and should be kept for modeling.\n",
    "\n",
    "---\n",
    "\n",
    "## Feature Selection Opinion\n",
    "\n",
    "-Keep  `engineSize`, `paintQuality%`, `previousOwners`:  Low correlation (independent features)\n",
    "\n",
    "---\n",
    "\n",
    "## Feature Engineering Opinion\n",
    "\n",
    "- **CarAge** (`2020 - year`):  Converts `year` into a more interpretable and continuous measure of vehicle age \n",
    "- **UsageRate** (`mileage / CarAge`): Represents how much the car is driven per year — better proxy for wear and tear \n",
    "- **EfficiencyScore** (`mpg / tax`): Combines efficiency and cost factors — higher efficiency and lower taxes leds to a higher price\n",
    "- **OwnerTurnover** (`previousOwners / CarAge`): Indicates how frequently ownership changes — may relate to reliability or desirability \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Fall2526",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
