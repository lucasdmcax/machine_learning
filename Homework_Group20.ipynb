{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "891cffd7",
   "metadata": {},
   "source": [
    "# Machine Learning Project\n",
    "\n",
    "## Table of Contents\n",
    "- [Import Data](#import-data)\n",
    "  - [Import Data Summary](#import-data-summary)\n",
    "- [Data Exploration](#data-exploration)\n",
    "  - [Boolean Features](#boolean-features)\n",
    "    - [Boolean Features Analysis](#boolean-features-analysis)\n",
    "  - [Categorical Features](#categorical-features)\n",
    "    - [Check Categorical Features Consistency](#check-categorical-features-consistency)\n",
    "    - [Categorical Features Summary](#categorical-features-summary)\n",
    "  - [Numerical Features](#numerical-features)\n",
    "    - [Numerical Plots](#plots)\n",
    "    - [Analysis of Numerical Distributions](#analysis-of-numerical-distributions)\n",
    "- [Pre-processing](#pre-processing)\n",
    "  - [Summary of Preprocessing Pipeline](#preprocessing-pipeline-summary)\n",
    "  - [Data Preparation](#data-preparation)\n",
    "    - [Correlation Analysis](#correlation-analysis)\n",
    "- [Model Training](#model-training)\n",
    "  - [Model Selection with Cross-Validation](#model-selection-with-cv)\n",
    "  - [Quick Baseline Model](#quick-baseline-model)\n",
    "  - [Experiment Algorithms](#experiment-algorithms)\n",
    "- [Predictions](#predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47693163",
   "metadata": {},
   "source": [
    "<a id=\"import-data\"></a>\n",
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9c6c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from scipy.stats import loguniform, randint\n",
    "from model_training_utils import (\n",
    "    general_cleaning,\n",
    "    preprocess_data,\n",
    "    cross_validate_with_tuning,\n",
    "    preprocess_test_data\n",
    ")\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6468f612",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv').set_index('carID')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc133ea3-1850-4386-ab5e-6db9950c48f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a5a91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_duplicated_ids = df.index.duplicated().sum()\n",
    "print(f'Number of duplicated carIDs: {num_duplicated_ids}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c607395",
   "metadata": {},
   "source": [
    "<a id=\"import-data-summary\"></a>\n",
    "#### Import Data Summary\n",
    "- Dataset loaded successfully with `carID` as the index\n",
    "- There are no duplicate entries in carID\n",
    "- The dataset contains information about cars including both numerical features (price, mileage, tax, etc.) and categorical features (brand, model, transmission, etc.)\n",
    "- Initial inspection shows multiple features that will require preprocessing:\n",
    "  - Numerical features that need cleaning (negative values, outliers)\n",
    "  - Categorical features that need standardization\n",
    "  - Presence of missing values in several columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3db49c",
   "metadata": {},
   "source": [
    "<a id=\"data-exploration\"></a>\n",
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb63f270",
   "metadata": {},
   "source": [
    "<a id=\"boolean-features\"></a>\n",
    "### Boolean Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e5927d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hasDamage'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb51349",
   "metadata": {},
   "source": [
    "<a id=\"boolean-features-analysis\"></a>\n",
    "#### Boolean Features Analysis\n",
    "\n",
    "Key observations about `hasDamage` feature:\n",
    "- Only contains binary values (0) and NaN\n",
    "- No instances of value 1 found, suggesting potential data collection issues\n",
    "- May indicate:\n",
    "  - Cars with damage not being listed\n",
    "  - System default setting of 0 for non-damaged cars\n",
    "  - Incomplete damage assessment process\n",
    "- Requires special handling in preprocessing:\n",
    "  - Consider treating NaN as a separate category\n",
    "  - Validate if 0 truly represents \"no damage\"\n",
    "  - May need to be treated as a categorical rather than boolean feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0273ca",
   "metadata": {},
   "source": [
    "<a id=\"categorical-features\"></a>\n",
    "### Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4432c021",
   "metadata": {},
   "source": [
    "<a id=\"check-categorical-features-consistency\"></a>\n",
    "#### Check Categorical Features Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095f1162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of categorical features\n",
    "cat_cols = ['Brand', 'model', 'fuelType', 'transmission']\n",
    "\n",
    "# Identify outlier examples in categorical features\n",
    "cat_outliers_examples = {col: df[col].value_counts().tail(10).index for col in cat_cols}\n",
    "\n",
    "# Display the outlier examples\n",
    "pd.DataFrame(cat_outliers_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa457064",
   "metadata": {},
   "source": [
    "<a id=\"categorical-features-summary\"></a>\n",
    "#### Categorical Features Summary\n",
    "- Initial analysis reveals significant data quality issues across all categorical columns\n",
    "- No standardization in categorical features, with multiple variations of the same values (different spellings, capitalizations)\n",
    "- Solution: We will implement string distance-based standardization using the `nltk` library to clean and standardize these features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ab1e0a",
   "metadata": {},
   "source": [
    "<a id=\"numerical-features\"></a>\n",
    "### Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6a26e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict of numerical features\n",
    "num_cols = {\n",
    "    'price': 'continuous',\n",
    "    'mileage': 'continuous',\n",
    "    'tax': 'continuous',\n",
    "    'mpg': 'continuous',\n",
    "    'paintQuality%': 'continuous',\n",
    "    'engineSize': 'continuous',\n",
    "    'year': 'discrete',\n",
    "    'previousOwners': 'discrete'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15d6909",
   "metadata": {},
   "source": [
    "<a id=\"plots\"></a>\n",
    "#### Numerical Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d22957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot figures for numerical features and the target variable (price)\n",
    "plt.figure(figsize=(16, 10))\n",
    "for i, (col, var_type) in enumerate(num_cols.items(), 1):\n",
    "    plt.subplot(4, 2, i)\n",
    "\n",
    "    # Plot based on variable type\n",
    "    if var_type == 'continuous':\n",
    "        sns.histplot(data=df, x=col, kde=True, color=\"lightcoral\", bins=30)\n",
    "        plt.title(f\"Distribution of {col}\", fontsize=11)\n",
    "    elif var_type == 'discrete':\n",
    "        sns.countplot(data=df, x=col, color=\"lightcoral\")\n",
    "        plt.title(f\"Distribution of {col}\", fontsize=11)\n",
    "        plt.xticks(rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f095a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots for continuous numerical features and the target variable (price)\n",
    "continuous_cols = [col for col, var_type in num_cols.items() if var_type == 'continuous']\n",
    "plt.figure(figsize=(16, 10))\n",
    "for i, col in enumerate(continuous_cols, 1):\n",
    "    plt.subplot(3, 2, i)\n",
    "    sns.boxplot(data=df, x=col, color=\"lightblue\")\n",
    "    plt.title(f\"Distribution of {col}\", fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0576a8e8",
   "metadata": {},
   "source": [
    "<a id=\"analysis-of-numerical-distributions\"></a>\n",
    "#### Analysis of Numerical Distributions\n",
    "\n",
    "Key observations from the plots:\n",
    "- **Target Variable (Price)**:\n",
    "  - Highly right-skewed distribution\n",
    "  - Contains significant number of outliers in the upper range\n",
    "  - Most cars are concentrated in the lower price range\n",
    "\n",
    "- **Mileage**:\n",
    "  - Right-skewed distribution\n",
    "  - Large range from nearly new cars to high-mileage vehicles\n",
    "  - Some outliers in upper range suggesting possible data entry errors\n",
    "\n",
    "- **Tax**:\n",
    "  - Multiple peaks suggesting different tax bands\n",
    "  - Contains negative values which require investigation (possible tax benefits/rebates)\n",
    "  - Large number of outliers on both ends of the distribution\n",
    "\n",
    "- **MPG (Miles Per Gallon)**:\n",
    "  - Approximately normal distribution with slight right skew\n",
    "  - Some unrealistic extreme values that need cleaning\n",
    "  - Reasonable median around typical car efficiency ranges\n",
    "\n",
    "- **Paint Quality %**:\n",
    "  - Contains values above 100% which are logically impossible\n",
    "  - Left-skewed distribution suggesting optimistic ratings\n",
    "  - Requires standardization to 0-100 range\n",
    "\n",
    "- **Engine Size**:\n",
    "  - There are engine size with zero values which are not realistic (might indicate electric vehicles)\n",
    "  - Some unusual patterns that need investigation\n",
    "  - Contains outliers that may represent specialty vehicles\n",
    "\n",
    "- **Year**:\n",
    "  - Should be discrete but contains decimal values\n",
    "\n",
    "- **Previous Owners**:\n",
    "  - Should be integer but contains float values\n",
    "  - Right-skewed distribution as expected\n",
    "  - Maximum values need validation (unusually high number of previous owners)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f2c3cb",
   "metadata": {},
   "source": [
    "<a id=\"pre-processing\"></a>\n",
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446f22c3",
   "metadata": {},
   "source": [
    "<a id=\"preprocessing-pipeline-summary\"></a>\n",
    "## Summary of Preprocessing Pipeline\n",
    "\n",
    "The preprocessing is now properly separated:\n",
    "\n",
    "1. **`preprocess_data()`** - Preprocesses a single dataset\n",
    "   - Handles categorical features (standardization, encoding)\n",
    "   - Handles numerical outliers using IQR method\n",
    "   - Imputes missing values with medians\n",
    "   - One-hot encodes categorical features\n",
    "   - Normalizes numerical features with StandardScaler\n",
    "   - Can fit transformers (fit=True) or use existing ones (fit=False)\n",
    "\n",
    "2. **`cross_validate_with_tuning()`** - Performs CV with hyperparameter tuning\n",
    "   - Takes **raw data** (after general_cleaning)\n",
    "   - Applies preprocessing **separately for each fold** (prevents data leakage)\n",
    "   - Performs manual hyperparameter search by sampling from parameter distributions\n",
    "   - Evaluates each combination on validation fold and tracks train/validation performance\n",
    "   - Returns best model configurations\n",
    "\n",
    "3. **`preprocess_test_data()`** - Preprocesses test data\n",
    "   - Uses artifacts from CV to ensure consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e972a0",
   "metadata": {},
   "source": [
    "<a id=\"data-preparation\"></a>\n",
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9194c9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare cleaned data for cross-validation\n",
    "df_cleaned = general_cleaning(df)\n",
    "X = df_cleaned.drop(columns=[\"price\"])\n",
    "y = df_cleaned[\"price\"]\n",
    "\n",
    "# Remove 'price' from num_cols since it's the target\n",
    "del num_cols['price']\n",
    "\n",
    "print(f\"Dataset size: {X.shape}\")\n",
    "print(f\"Target range: £{y.min():.2f} - £{y.max():.2f}\")\n",
    "print(f\"\\nReady for cross-validation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cc5ab5",
   "metadata": {},
   "source": [
    "<a id=\"correlation-analysis\"></a>\n",
    "#### Correlation Analysis\n",
    "\n",
    "Before model training, let's examine correlations between numerical features to understand their relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4e09b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for numerical features\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "corr = X[list(num_cols.keys())].corr(method=\"pearson\")\n",
    "sns.heatmap(data=corr, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Matrix of Numerical Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f115e4",
   "metadata": {},
   "source": [
    "<a id=\"model-training\"></a>\n",
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfc3b63",
   "metadata": {},
   "source": [
    "<a id=\"model-selection-with-cv\"></a>\n",
    "### Model Selection with Cross-Validation\n",
    "\n",
    "We'll use cross-validation with hyperparameter tuning to select the best model. Configure your model using a dictionary with the model class, parameter distributions, and number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e1e089",
   "metadata": {},
   "source": [
    "<a id=\"quick-baseline-model\"></a>\n",
    "### Quick Baseline Model\n",
    "\n",
    "Before running extensive CV, let's train a simple baseline model for quick reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a55e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick train/val split for baseline\n",
    "X_train_baseline, X_val_baseline, y_train_baseline, y_val_baseline = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=SEED\n",
    ")\n",
    "\n",
    "# Preprocess baseline data\n",
    "X_train_processed, baseline_artifacts = preprocess_data(X_train_baseline, cat_cols, num_cols, fit=True)\n",
    "X_val_processed = preprocess_data(X_val_baseline, cat_cols, num_cols, artifacts=baseline_artifacts, fit=False)\n",
    "\n",
    "# Train simple Ridge model\n",
    "baseline_model = Ridge(alpha=1.0, fit_intercept=True)\n",
    "baseline_model.fit(X_train_processed, np.log1p(y_train_baseline))\n",
    "\n",
    "# Evaluate\n",
    "y_train_pred = np.expm1(baseline_model.predict(X_train_processed))\n",
    "y_val_pred = np.expm1(baseline_model.predict(X_val_processed))\n",
    "\n",
    "mae_train = mean_absolute_error(y_train_baseline, y_train_pred)\n",
    "mae_val = mean_absolute_error(y_val_baseline, y_val_pred)\n",
    "r2_val = r2_score(y_val_baseline, y_val_pred)\n",
    "\n",
    "print(f\"Baseline Ridge (alpha=1.0):\")\n",
    "print(f\"  Train MAE: £{mae_train:.2f}\")\n",
    "print(f\"  Val MAE:   £{mae_val:.2f}\")\n",
    "print(f\"  Val R²:    {r2_val:.4f}\")\n",
    "print(f\"\\nThis gives us a reference point before hyperparameter tuning with CV.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5747bc65",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbd90bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importance(model_class, X_train, y_train):\n",
    "   \n",
    "    # Trees\n",
    "    criteria_map = {\n",
    "        RandomForestRegressor: [\"squared_error\", \"absolute_error\", \"poisson\"],\n",
    "        ExtraTreesRegressor:   [\"squared_error\", \"absolute_error\", \"poisson\"],\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "\n",
    "    if model_class in criteria_map:\n",
    "        for crit in criteria_map[model_class]:\n",
    "            model = model_class(criterion=crit)\n",
    "            model.fit(X_train, y_train)\n",
    "            importance = model.feature_importances_\n",
    "            results.append((crit, importance))\n",
    "\n",
    "    # Gradient boosting\n",
    "    elif model_class is GradientBoostingRegressor:\n",
    "        model = model_class()\n",
    "        model.fit(X_train, y_train)\n",
    "        results.append((\"feature_importances\", model.feature_importances_))\n",
    "\n",
    "    # Ridge / Lasso (based on coef)\n",
    "    elif model_class in [Ridge, Lasso]:\n",
    "        model = model_class()\n",
    "        model.fit(X_train, y_train)\n",
    "        coef = np.abs(model.coef_)\n",
    "        if coef.ndim > 1:  # multioutput\n",
    "            coef = coef.mean(axis=0)\n",
    "        results.append((\"coef\", coef))\n",
    "\n",
    "    # MLP \n",
    "    elif model_class is MLPRegressor:\n",
    "        model = model_class(max_iter=2000)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Importance based on first layer weights\n",
    "        first_layer_weights = model.coefs_[0]     # shape = (n_features, n_hidden)\n",
    "        importance = np.linalg.norm(first_layer_weights, axis=1)\n",
    "\n",
    "        results.append((\"mlp_weights\", importance))\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Model {model_class.__name__} not supported.\")\n",
    "\n",
    "    df_list = []\n",
    "    for label, values in results:\n",
    "        df_list.append(pd.DataFrame({\n",
    "            \"Feature\": X_train.columns,\n",
    "            \"Value\": values,\n",
    "            \"Method\": label\n",
    "        }))\n",
    "\n",
    "    tidy = pd.concat(df_list)\n",
    "    tidy.sort_values(\"Value\", ascending=False, inplace=True)\n",
    "\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    sns.barplot(data=tidy, y=\"Feature\", x=\"Value\", hue=\"Method\")\n",
    "    plt.title(f\"Feature Importance — {model_class.__name__}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb601b4",
   "metadata": {},
   "source": [
    "<a id=\"experiment-algorithms\"></a>\n",
    "### Experiment Algorithms\n",
    "\n",
    "Now we'll experiment with different algorithms using cross-validation with hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab6b358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Ridge Regression with hyperparameter tuning\n",
    "ridge_config = {\n",
    "    'model_class': Ridge,\n",
    "    'param_distributions': {\n",
    "        'alpha': loguniform(1e-3, 1e2),\n",
    "        'fit_intercept': [True, False]\n",
    "    },\n",
    "    'n_iter': 20\n",
    "}\n",
    "\n",
    "ridge_results = cross_validate_with_tuning(X, y, cat_cols, num_cols, ridge_config, k=3, seed=SEED)\n",
    "get_feature_importance(Ridge, X_train_processed, y_train_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ebdfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Lasso Regression\n",
    "lasso_config = {\n",
    "    'model_class': Lasso,\n",
    "    'param_distributions': {\n",
    "        'alpha': loguniform(1e-3, 1e2),\n",
    "        'fit_intercept': [True, False]\n",
    "    },\n",
    "    'n_iter': 20\n",
    "}\n",
    "\n",
    "lasso_results = cross_validate_with_tuning(X, y, cat_cols, num_cols, lasso_config, k=3, seed=SEED)\n",
    "get_feature_importance(Lasso, X_train_processed, y_train_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8fb232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Random Forest\n",
    "rf_config = {\n",
    "    'model_class': RandomForestRegressor,\n",
    "    'param_distributions': {\n",
    "        'n_estimators': randint(10, 50),\n",
    "        'max_depth': randint(5, 15),\n",
    "        'min_samples_split': randint(20, 50),\n",
    "        'min_samples_leaf': randint(5, 15),\n",
    "        'max_features': ['sqrt', 0.5],\n",
    "    },\n",
    "    'n_iter': 10\n",
    "}\n",
    "\n",
    "rf_results = cross_validate_with_tuning(X, y, cat_cols, num_cols, rf_config, k=3, seed=SEED)\n",
    "get_feature_importance(RandomForestRegressor, X_train_processed, y_train_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45536493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Gradient Boosting Regressor\n",
    "gb_config = {\n",
    "    'model_class': GradientBoostingRegressor,\n",
    "    'param_distributions': {\n",
    "        'n_estimators': randint(50, 100),\n",
    "        'learning_rate': loguniform(0.01, 0.2),\n",
    "        'max_depth': randint(3, 8),\n",
    "        'min_samples_split': randint(2, 20),\n",
    "        'min_samples_leaf': randint(1, 10),\n",
    "        'subsample': [0.8, 0.9, 1.0],\n",
    "    },\n",
    "    'n_iter': 20\n",
    "}\n",
    "\n",
    "gb_results = cross_validate_with_tuning(X, y, cat_cols, num_cols, gb_config, k=3, seed=SEED)\n",
    "get_feature_importance(GradientBoostingRegressor, X_train_processed, y_train_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4740b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 Example: Extra Trees Regressor\n",
    "et_config = {\n",
    "    'model_class': ExtraTreesRegressor,\n",
    "    'param_distributions': {\n",
    "        'n_estimators': randint(1, 5),\n",
    "        'max_depth': randint(3, 20),\n",
    "        'min_samples_split': randint(10, 100),\n",
    "        'min_samples_leaf': randint(1, 10),\n",
    "    },\n",
    "    'n_iter': 20\n",
    "}\n",
    "\n",
    "et_results = cross_validate_with_tuning(X, y, cat_cols, num_cols, et_config, k=3, seed=SEED)\n",
    "get_feature_importance(ExtraTreesRegressor, X_train_processed, y_train_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787dea3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 6: MLP Regressor (Neural Network)\n",
    "mlp_config = {\n",
    "    'model_class': MLPRegressor,\n",
    "    'param_distributions': {\n",
    "        'hidden_layer_sizes': [(16,), (32,), (64,), (16, 8), (32, 16), (64, 32)],\n",
    "        'activation': ['relu', 'tanh'],\n",
    "        'solver': 'adam',\n",
    "        'alpha': loguniform(1e-5, 1e-1),\n",
    "        'learning_rate_init': loguniform(1e-4, 1e-2),\n",
    "        'max_iter': 500,\n",
    "        'early_stopping': True\n",
    "    },\n",
    "    'n_iter': 5 \n",
    "}\n",
    "\n",
    "mlp_results = cross_validate_with_tuning(X, y, cat_cols, num_cols, mlp_config, k=3, seed=SEED)\n",
    "get_feature_importance(MLPRegressor, X_train_processed, y_train_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d70f1b",
   "metadata": {},
   "source": [
    "### Final Model Selection\n",
    "\n",
    "I will select the best algorithm with the best hyperparameters and train the model with it. While doing it I want to use all available data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652a272f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models to find the best one\n",
    "results = {\n",
    "    'Ridge': ridge_results,\n",
    "    'Lasso': lasso_results,\n",
    "    'Random Forest': rf_results,\n",
    "    'Extra Trees': et_results,\n",
    "    'MLP': mlp_results,\n",
    "    'Gradient Boosting': gb_results\n",
    "}\n",
    "\n",
    "# Select the best model based on the lowest mean CV score (MAE)\n",
    "best_model_name = min(results, key=lambda k: results[k]['mean_cv_score'])\n",
    "best_result = results[best_model_name]\n",
    "\n",
    "print(f\"Best Model Selected: {best_model_name}\")\n",
    "print(f\"Best CV MAE: £{best_result['mean_cv_score']:.2f} ± £{best_result['std_cv_score']:.2f}\")\n",
    "print(f\"Best Parameters: {best_result['best_params']}\\n\")\n",
    "\n",
    "# Retrain the best model on ALL available data\n",
    "print(f\"Retraining {best_model_name} on all available data...\")\n",
    "\n",
    "# 1. Preprocess the entire dataset\n",
    "X_all_processed, preprocessing_artifacts = preprocess_data(X, cat_cols, num_cols, fit=True)\n",
    "\n",
    "# 2. Prepare the target variable (log transform)\n",
    "y_all_log = np.log1p(y)\n",
    "\n",
    "# 3. Get the model with the best parameters\n",
    "# The function returns an unfitted model with the best params, so we can use it directly\n",
    "best_model = best_result['best_estimator']\n",
    "\n",
    "# 4. Train the model on the full dataset\n",
    "best_model.fit(X_all_processed, y_all_log)\n",
    "\n",
    "print(\"Final model trained successfully on all data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb360862",
   "metadata": {},
   "source": [
    "<a id=\"predictions\"></a>\n",
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb33e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess test data\n",
    "test_df = pd.read_csv('data/test.csv').set_index('carID')\n",
    "test_processed = preprocess_test_data(test_df, preprocessing_artifacts)\n",
    "\n",
    "# Make predictions\n",
    "test_predictions = np.expm1(best_model.predict(test_processed))\n",
    "\n",
    "# Save predictions\n",
    "predictions_df = pd.DataFrame({'price': test_predictions}, index=test_df.index)\n",
    "predictions_df.to_csv('data/test_predictions.csv')\n",
    "\n",
    "print(f\"Predictions saved for {len(test_predictions)} test samples\")\n",
    "print(f\"Predicted price range: £{test_predictions.min():.2f} - £{test_predictions.max():.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataMining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
