{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "891cffd7",
   "metadata": {},
   "source": [
    "# Machine Learning Project\n",
    "\n",
    "<details>\n",
    "<summary><strong>Table of Contents</strong> (Click to expand)</summary>\n",
    "\n",
    "- [I. Identifying Business Needs](#business-needs)\n",
    "- [II. Data Exploration and Preprocessing](#data-exploration)\n",
    "  - [Boolean Features](#boolean-features)\n",
    "  - [Categorical Features](#categorical-features)\n",
    "  - [Numerical Features](#numerical-features)\n",
    "  - [Preprocessing Pipeline](#preprocessing)\n",
    "- [III. Regression Benchmarking](#benchmarking)\n",
    "  - [Model Selection Strategy](#model-selection-strategy)\n",
    "  - [Quick Baseline Model](#quick-baseline-model)\n",
    "- [IV. Open-Ended Section: Advanced Search](#open-ended)\n",
    "  - [Experiment Algorithms](#experiment-algorithms)\n",
    "  - [Final Model Selection](#final-model-selection)\n",
    "- [V. Deployment](#deployment)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd1cee6",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "### Context:\n",
    "This project addresses car price prediction—a fundamental regression task in machine learning. The dataset includes features spanning categorical attributes (brand, model, transmission type, fuel type) and numerical characteristics (mileage, engine size, tax, MPG, paint quality). This problem is relevant for automotive valuations, insurance pricing, and market analysis.\n",
    "\n",
    "### Objectives:\n",
    "The primary goals were to:\n",
    "1.  Systematically explore and preprocess complex, real-world automotive data containing missing values, outliers, and inconsistencies.\n",
    "2.  Develop a robust preprocessing pipeline that prevents data leakage through proper fold-wise application.\n",
    "3.  Benchmark multiple regression algorithms (Linear, Ensemble, Neural Networks) with hyperparameter tuning via cross-validation.\n",
    "4.  Identify the most influential features through importance analysis.\n",
    "\n",
    "### Methodology:\n",
    "Data exploration revealed categorical inconsistencies (typos) and numerical anomalies. The preprocessing pipeline incorporated:\n",
    "*   **Data Cleaning**: Handling outliers (IQR) and standardizing categorical values (Edit Distance).\n",
    "*   **Feature Engineering**: Transforming `year` to `age` and log-transforming skewed features like `mileage`.\n",
    "*   **Encoding**: Using **Target Encoding** for high-cardinality features (`model`) and One-Hot Encoding for others.\n",
    "*   **Ablation Analysis**: We treated preprocessing steps as hyperparameters, training approximately **780 models** across 8 different algorithms to find the optimal combination of pipeline and model parameters.\n",
    "\n",
    "### Main Results:\n",
    "Initial benchmarking with **Ridge Regression** established a baseline and demonstrated that **log-transforming the target variable** is critical (improving MAE by ~£400). The extensive search revealed that **HistGradientBoostingRegressor** achieved the best performance, outperforming both linear models and other ensemble methods.\n",
    "\n",
    "### Conclusions:\n",
    "The project demonstrates that systematic preprocessing is as critical as algorithm choice. While **log-transforming the target** was essential for linear models, our ablation analysis on **ensemble methods** showed mixed results, with top-performing configurations appearing both with and without the transformation. Ultimately, the **HistGradientBoostingRegressor** achieved the best performance, highlighting the importance of handling non-linear relationships in pricing data. The final model is selected based on rigorous cross-validation to ensure generalizability to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47693163",
   "metadata": {},
   "source": [
    "<a id=\"business-needs\"></a>\n",
    "## I. Identifying Business Needs\n",
    "\n",
    "### Overview and Goals\n",
    "The primary objective of this project is to build a robust machine learning model to predict the price of used cars based on their attributes (brand, model, mileage, etc.). Accurate price prediction is essential for:\n",
    "- **Dealerships**: To set competitive yet profitable prices.\n",
    "- **Buyers**: To assess fair market value.\n",
    "- **Insurance Companies**: To estimate vehicle value for premiums and claims.\n",
    "\n",
    "### Process Description\n",
    "Our workflow follows a structured data science lifecycle:\n",
    "1.  **Data Exploration**: Understanding the distribution and quality of the data.\n",
    "2.  **Preprocessing**: Cleaning data and preparing it for modeling (handling missing values, outliers, encoding).\n",
    "3.  **Benchmarking**: Establishing a baseline with simple linear models.\n",
    "4.  **Advanced Search**: Exploring complex algorithms and feature subsets.\n",
    "5.  **Deployment**: Generating predictions for the test set.\n",
    "\n",
    "### Assessment Approach\n",
    "We employ **K-Fold Cross-Validation (K=3)** as our primary assessment strategy.\n",
    "- **Metric**: Mean Absolute Error (MAE) is used as the primary metric because it is interpretable (average error in £).\n",
    "- **Validation**: We strictly separate training and validation data within each fold to prevent leakage.\n",
    "\n",
    "### Computational Constraints & Demo Mode\n",
    "Given the complexity of the pipeline (preprocessing inside CV loops) and the size of the dataset, a full hyperparameter search is computationally expensive.\n",
    "- **Full Mode**: Runs 30-50 iterations of random search per algorithm to find the absolute best configuration.\n",
    "- **Demo Mode**: skips the most expensive models (like Random Forest) to allow for quick pipeline verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9c6c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from scipy.stats import loguniform, randint\n",
    "\n",
    "# Import custom utility functions\n",
    "from model_training_utils import (\n",
    "    general_cleaning, \n",
    "    preprocess_data,\n",
    "    get_feature_importance,\n",
    "    cross_validate_with_tuning,\n",
    "    preprocess_test_data\n",
    ")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Configuration\n",
    "DEMO_MODE = False # Set to True to run a quick demo with fewer iterations\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6468f612",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv').set_index('carID')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc133ea3-1850-4386-ab5e-6db9950c48f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a5a91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. General Cleaning\n",
    "df_cleaned = general_cleaning(df)\n",
    "\n",
    "# Separate features and target\n",
    "X = df_cleaned.drop(columns=['price'])\n",
    "y = df_cleaned['price']\n",
    "\n",
    "print(f\"Cleaned dataset shape: {df_cleaned.shape}\")\n",
    "print(f\"Features shape: {X.shape}, Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c607395",
   "metadata": {},
   "source": [
    "<a id=\"import-data-summary\"></a>\n",
    "#### Import Data Summary\n",
    "- Dataset loaded successfully with `carID` as the index\n",
    "- There are no duplicate entries in carID\n",
    "- The dataset contains information about cars including both numerical features (price, mileage, tax, etc.) and categorical features (brand, model, transmission, etc.)\n",
    "- Initial inspection shows multiple features that will require preprocessing:\n",
    "  - Numerical features that need cleaning (negative values, outliers)\n",
    "  - Categorical features that need standardization\n",
    "  - Presence of missing values in several columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3db49c",
   "metadata": {},
   "source": [
    "<a id=\"data-exploration\"></a>\n",
    "## II. Data Exploration and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb63f270",
   "metadata": {},
   "source": [
    "<a id=\"boolean-features\"></a>\n",
    "### Boolean Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e5927d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hasDamage'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb51349",
   "metadata": {},
   "source": [
    "<a id=\"boolean-features-analysis\"></a>\n",
    "#### Boolean Features Analysis\n",
    "\n",
    "Key observations about `hasDamage` feature:\n",
    "- Only contains binary values (0) and NaN\n",
    "- No instances of value 1 found, suggesting potential data collection issues\n",
    "- May indicate:\n",
    "  - Cars with damage not being listed\n",
    "  - System default setting of 0 for non-damaged cars\n",
    "  - Incomplete damage assessment process\n",
    "- Requires special handling in preprocessing:\n",
    "  - Consider treating NaN as a separate category\n",
    "  - Validate if 0 truly represents \"no damage\"\n",
    "  - May need to be treated as a categorical rather than boolean feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0273ca",
   "metadata": {},
   "source": [
    "<a id=\"categorical-features\"></a>\n",
    "### Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4432c021",
   "metadata": {},
   "source": [
    "<a id=\"check-categorical-features-consistency\"></a>\n",
    "#### Check Categorical Features Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095f1162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of categorical features\n",
    "cat_cols = ['Brand', 'model', 'fuelType', 'transmission']\n",
    "\n",
    "# Identify outlier examples in categorical features\n",
    "cat_outliers_examples = {col: df[col].value_counts().tail(10).index for col in cat_cols}\n",
    "\n",
    "# Display the outlier examples\n",
    "pd.DataFrame(cat_outliers_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa457064",
   "metadata": {},
   "source": [
    "<a id=\"categorical-features-summary\"></a>\n",
    "#### Categorical Features Summary\n",
    "- Initial analysis reveals significant data quality issues across all categorical columns\n",
    "- No standardization in categorical features, with multiple variations of the same values (different spellings, capitalizations)\n",
    "- Solution: We will implement string distance-based standardization using the `nltk` library to clean and standardize these features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ab1e0a",
   "metadata": {},
   "source": [
    "<a id=\"numerical-features\"></a>\n",
    "### Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6a26e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict of numerical features\n",
    "num_cols = {\n",
    "    'price': 'continuous',\n",
    "    'mileage': 'continuous',\n",
    "    'tax': 'continuous',\n",
    "    'mpg': 'continuous',\n",
    "    'paintQuality%': 'continuous',\n",
    "    'engineSize': 'continuous',\n",
    "    'year': 'discrete',\n",
    "    'previousOwners': 'discrete'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15d6909",
   "metadata": {},
   "source": [
    "<a id=\"plots\"></a>\n",
    "#### Numerical Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d22957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot figures for numerical features and the target variable (price)\n",
    "plt.figure(figsize=(16, 10))\n",
    "for i, (col, var_type) in enumerate(num_cols.items(), 1):\n",
    "    plt.subplot(4, 2, i)\n",
    "\n",
    "    # Plot based on variable type\n",
    "    if var_type == 'continuous':\n",
    "        sns.histplot(data=df, x=col, kde=True, color=\"lightcoral\", bins=30)\n",
    "        plt.title(f\"Distribution of {col}\", fontsize=11)\n",
    "    elif var_type == 'discrete':\n",
    "        sns.countplot(data=df, x=col, color=\"lightcoral\")\n",
    "        plt.title(f\"Distribution of {col}\", fontsize=11)\n",
    "        plt.xticks(rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f095a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots for continuous numerical features and the target variable (price)\n",
    "continuous_cols = [col for col, var_type in num_cols.items() if var_type == 'continuous']\n",
    "plt.figure(figsize=(16, 10))\n",
    "for i, col in enumerate(continuous_cols, 1):\n",
    "    plt.subplot(3, 2, i)\n",
    "    sns.boxplot(data=df, x=col, color=\"lightblue\")\n",
    "    plt.title(f\"Distribution of {col}\", fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0576a8e8",
   "metadata": {},
   "source": [
    "<a id=\"analysis-of-numerical-distributions\"></a>\n",
    "#### Analysis of Numerical Distributions\n",
    "\n",
    "Key observations from the plots:\n",
    "- **Target Variable (Price)**:\n",
    "  - Highly right-skewed distribution\n",
    "  - Contains significant number of outliers in the upper range\n",
    "  - Most cars are concentrated in the lower price range\n",
    "\n",
    "- **Mileage**:\n",
    "  - Right-skewed distribution. **Action**: We will apply a log-transformation to linearize this relationship with price.\n",
    "  - Large range from nearly new cars to high-mileage vehicles\n",
    "  - Some outliers in upper range suggesting possible data entry errors\n",
    "\n",
    "- **Tax**:\n",
    "  - Multiple peaks suggesting different tax bands\n",
    "  - Contains negative values which require investigation (possible tax benefits/rebates)\n",
    "  - Large number of outliers on both ends of the distribution\n",
    "\n",
    "- **MPG (Miles Per Gallon)**:\n",
    "  - Approximately normal distribution with slight right skew\n",
    "  - Some unrealistic extreme values that need cleaning\n",
    "  - Reasonable median around typical car efficiency ranges\n",
    "\n",
    "- **Paint Quality %**:\n",
    "  - Contains values above 100% which are logically impossible\n",
    "  - Left-skewed distribution suggesting optimistic ratings\n",
    "  - Requires standardization to 0-100 range\n",
    "\n",
    "- **Engine Size**:\n",
    "  - There are engine size with zero values which are not realistic (might indicate electric vehicles)\n",
    "  - Some unusual patterns that need investigation\n",
    "  - Contains outliers that may represent specialty vehicles\n",
    "\n",
    "- **Year**:\n",
    "  - Should be discrete but contains decimal values\n",
    "\n",
    "- **Previous Owners**:\n",
    "  - Should be integer but contains float values\n",
    "  - Right-skewed distribution as expected\n",
    "  - Maximum values need validation (unusually high number of previous owners)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f2c3cb",
   "metadata": {},
   "source": [
    "<a id=\"preprocessing\"></a>\n",
    "### Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446f22c3",
   "metadata": {},
   "source": [
    "<a id=\"preprocessing-pipeline-summary\"></a>\n",
    "## Summary of Preprocessing Pipeline\n",
    "\n",
    "We have implemented a comprehensive preprocessing pipeline (`preprocess_data`) that handles the specific characteristics of this dataset:\n",
    "\n",
    "1.  **Feature Engineering & Transformations**:\n",
    "    *   **Log Transformation**: Applied to `mileage` to handle its heavy right-skew.\n",
    "    *   **Age Calculation**: Converted `year` to `age` (2020 - year) to capture depreciation.\n",
    "\n",
    "2.  **Categorical Encoding**:\n",
    "    *   **Target Encoding**: Applied to the `model` feature. Since `model` has high cardinality (many unique values), One-Hot Encoding would create too many sparse features. Target Encoding maps each car model to the average price of that model (smoothed), capturing the signal efficiently.\n",
    "    *   **One-Hot Encoding**: Applied to low-cardinality features (`Brand`, `transmission`, `fuelType`).\n",
    "\n",
    "3.  **Data Cleaning**:\n",
    "    *   **Outlier Removal**: Uses the IQR method (tunable via `clean_outliers_flag`).\n",
    "    *   **Imputation**: Missing values are filled with medians from the training set.\n",
    "    *   **Standardization**: Numerical features are scaled using `StandardScaler` (`normalize_flag`).\n",
    "\n",
    "4.  **Leakage Prevention**:\n",
    "    *   All statistics (medians, target encoding maps, scalers) are learned *only* on the training split and applied to validation/test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e972a0",
   "metadata": {},
   "source": [
    "<a id=\"data-preparation\"></a>\n",
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9194c9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare cleaned data for cross-validation\n",
    "df_cleaned = general_cleaning(df)\n",
    "X = df_cleaned.drop(columns=[\"price\"])\n",
    "y = df_cleaned[\"price\"]\n",
    "\n",
    "# Remove 'price' from num_cols since it's the target\n",
    "del num_cols['price']\n",
    "\n",
    "print(f\"Dataset size: {X.shape}\")\n",
    "print(f\"Target range: £{y.min():.2f} - £{y.max():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cc5ab5",
   "metadata": {},
   "source": [
    "<a id=\"correlation-analysis\"></a>\n",
    "#### Correlation Analysis\n",
    "\n",
    "Before model training, let's examine correlations between numerical features to understand their relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4e09b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for numerical features\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "corr = X[list(num_cols.keys())].corr(method=\"pearson\")\n",
    "sns.heatmap(data=corr, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Matrix of Numerical Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f115e4",
   "metadata": {},
   "source": [
    "<a id=\"benchmarking\"></a>\n",
    "## III. Regression Benchmarking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfc3b63",
   "metadata": {},
   "source": [
    "<a id=\"model-selection-strategy\"></a>\n",
    "### Model Selection Strategy\n",
    "\n",
    "We employ a robust **Cross-Validation (CV)** strategy with **Randomized Hyperparameter Search** to select the best model.\n",
    "\n",
    "#### 1. Prevention of Data Leakage\n",
    "A critical aspect of our pipeline is that **preprocessing occurs inside the cross-validation loop**.\n",
    "- For every fold, statistics (medians, outlier bounds, categorical frequencies) are calculated *only* on the training portion.\n",
    "- These statistics are then applied to the validation portion.\n",
    "- This ensures that the validation score is a true estimate of performance on unseen data, as no information from the validation set leaks into the training process.\n",
    "\n",
    "#### 2. Dynamic Feature Selection\n",
    "We treat feature selection as a hyperparameter. Using `SelectKBest` with `f_regression`, we dynamically select the top $k$ features during training.\n",
    "- The parameter `feature_selection_k` controls how many features are kept.\n",
    "- By including this in the search space (e.g., `[10, 20, 30, None]`), the model can \"choose\" whether to use a subset of informative features or the full feature set.\n",
    "\n",
    "#### 3. Tunable Pipeline Parameters\n",
    "Beyond model-specific hyperparameters (like `alpha` for Ridge or `n_estimators` for Random Forest), we tune the preprocessing pipeline itself:\n",
    "- **`clean_outliers_flag`**: (True/False) Whether to remove outliers from numerical features using the IQR method.\n",
    "- **`standardize_cats_flag`**: (True/False) Whether to fix typos in categorical variables using edit distance.\n",
    "- **`normalize_flag`**: (True/False) Whether to scale numerical features (StandardScaler).\n",
    "- **`log_target`**: (True/False) Whether to train on $log(1 + price)$.\n",
    "    *   **Rationale**: The price distribution is highly right-skewed. Training on the raw scale causes models to over-penalize errors on expensive cars. Log-transforming the target makes the distribution more normal, stabilizing the variance and typically improving performance (MAE).\n",
    "\n",
    "#### 4. Configuration Structure\n",
    "Each experiment is defined by a configuration dictionary. Here is an example of how we define the search space:\n",
    "\n",
    "```python\n",
    "config = {\n",
    "    'model_class': RandomForestRegressor,\n",
    "    'param_distributions': {\n",
    "        # Model-specific parameters\n",
    "        'n_estimators': randint(100, 300),\n",
    "        'max_depth': [None, 10, 20],\n",
    "        \n",
    "        # Pipeline parameters\n",
    "        'clean_outliers_flag': [True, False],\n",
    "        'standardize_cats_flag': [True, False],\n",
    "        'log_target': [True, False],\n",
    "        \n",
    "        # Feature selection parameter\n",
    "        'feature_selection_k': [10, 20, 30, None] \n",
    "    },\n",
    "    'n_iter': 20  # Number of random combinations to try\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e1e089",
   "metadata": {},
   "source": [
    "<a id=\"quick-baseline-model\"></a>\n",
    "### Quick Baseline Model\n",
    "\n",
    "Before running extensive CV, we train a simple **Ridge Regression** model to establish a baseline and verify the impact of log-transforming the target.\n",
    "\n",
    "**Baseline Results Analysis:**\n",
    "Based on the initial run, we observed a significant performance gap:\n",
    "*   **With Log Transform**: Validation MAE $\\approx £2,376$ ($R^2 \\approx 0.83$)\n",
    "*   **Without Log Transform**: Validation MAE $\\approx £2,751$ ($R^2 \\approx 0.79$)\n",
    "\n",
    "**Conclusion**: The log-transformation of the target variable improves the Mean Absolute Error by nearly **£400**. This confirms that handling the skewness of the price variable is critical for this dataset. We will prioritize `log_target=True` in our advanced search but will keep it as a tunable parameter to be sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a55e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick train/val split for baseline\n",
    "X_train_baseline, X_val_baseline, y_train_baseline, y_val_baseline = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=SEED\n",
    ")\n",
    "\n",
    "# Preprocess baseline data\n",
    "X_train_processed, baseline_artifacts = preprocess_data(X_train_baseline, cat_cols, num_cols, fit=True, y=y_train_baseline)\n",
    "X_val_processed = preprocess_data(X_val_baseline, cat_cols, num_cols, artifacts=baseline_artifacts, fit=False)\n",
    "\n",
    "# 1. Baseline WITH Log Transform\n",
    "baseline_model_log = Ridge(alpha=1.0, fit_intercept=True)\n",
    "baseline_model_log.fit(X_train_processed, np.log1p(y_train_baseline))\n",
    "\n",
    "y_train_pred_log = np.expm1(baseline_model_log.predict(X_train_processed))\n",
    "y_val_pred_log = np.expm1(baseline_model_log.predict(X_val_processed))\n",
    "\n",
    "mae_train_log = mean_absolute_error(y_train_baseline, y_train_pred_log)\n",
    "mae_val_log = mean_absolute_error(y_val_baseline, y_val_pred_log)\n",
    "r2_val_log = r2_score(y_val_baseline, y_val_pred_log)\n",
    "\n",
    "print(f\"Baseline Ridge (WITH Log Transform):\")\n",
    "print(f\"  Train MAE: £{mae_train_log:.2f}\")\n",
    "print(f\"  Val MAE:   £{mae_val_log:.2f}\")\n",
    "print(f\"  Val R²:    {r2_val_log:.4f}\")\n",
    "\n",
    "# 2. Baseline WITHOUT Log Transform\n",
    "baseline_model_no_log = Ridge(alpha=1.0, fit_intercept=True)\n",
    "baseline_model_no_log.fit(X_train_processed, y_train_baseline)\n",
    "\n",
    "y_train_pred_no_log = baseline_model_no_log.predict(X_train_processed)\n",
    "y_val_pred_no_log = baseline_model_no_log.predict(X_val_processed)\n",
    "\n",
    "mae_train_no_log = mean_absolute_error(y_train_baseline, y_train_pred_no_log)\n",
    "mae_val_no_log = mean_absolute_error(y_val_baseline, y_val_pred_no_log)\n",
    "r2_val_no_log = r2_score(y_val_baseline, y_val_pred_no_log)\n",
    "\n",
    "print(f\"\\nBaseline Ridge (WITHOUT Log Transform):\")\n",
    "print(f\"  Train MAE: £{mae_train_no_log:.2f}\")\n",
    "print(f\"  Val MAE:   £{mae_val_no_log:.2f}\")\n",
    "print(f\"  Val R²:    {r2_val_no_log:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb601b4",
   "metadata": {},
   "source": [
    "<a id=\"open-ended\"></a>\n",
    "## IV. Open-Ended Section: Advanced Search & Ablation Analysis\n",
    "\n",
    "### Objectives\n",
    "In this section, we go beyond standard benchmarking by conducting a comprehensive **Ablation Analysis** integrated directly into our hyperparameter search. Our goal is to determine not just the best *algorithm*, but the optimal *preprocessing strategy* for each algorithm.\n",
    "\n",
    "### Methodology: Pipeline as a Hyperparameter\n",
    "Instead of fixing the preprocessing steps beforehand, we treat key preprocessing decisions as **tunable hyperparameters**. This allows us to perform an automated ablation study during Cross-Validation.\n",
    "\n",
    "For every model iteration, the randomized search selects:\n",
    "1.  **Preprocessing Flags** (Ablation):\n",
    "    *   `clean_outliers_flag`: Does removing outliers improve performance?\n",
    "    *   `standardize_cats_flag`: Does fixing typos in categorical data help?\n",
    "    *   `log_target`: Is log-transforming the price beneficial for this specific model?\n",
    "    *   `normalize_flag`: Does scaling features matter for tree-based models vs. linear models?\n",
    "2.  **Feature Subset** (Dynamic Selection):\n",
    "    *   `feature_selection_k`: How many top features ($k$) yield the best results?\n",
    "3.  **Model Hyperparameters**:\n",
    "    *   Standard algorithm-specific parameters (e.g., `n_estimators`, `alpha`).\n",
    "\n",
    "### Actions Taken\n",
    "We expand our search to include advanced algorithms—**Random Forest, Gradient Boosting, Extra Trees, and MLP**—while simultaneously optimizing the pipeline configuration for each.\n",
    "\n",
    "### Discussion of Findings\n",
    "The results below will reveal the optimal combination of **Algorithm + Preprocessing Strategy + Feature Subset**. This approach allows us to scientifically validate whether specific preprocessing steps (like outlier removal) actually contribute to model performance or if they are unnecessary for robust non-linear models like Random Forest.\n",
    "\n",
    "<a id=\"experiment-algorithms\"></a>\n",
    "### Experiment Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746c7538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 0: Linear Regression\n",
    "lr_config = {\n",
    "    'model_class': LinearRegression,\n",
    "    'param_distributions': {\n",
    "        'fit_intercept': [True, False],\n",
    "        'clean_outliers_flag': [True, False],\n",
    "        'standardize_cats_flag': [True, False],\n",
    "        'normalize_flag': [True, False],\n",
    "        'log_target': [True, False],\n",
    "        'feature_selection_k': [10, 20, 30, 40, 50, None]\n",
    "    },\n",
    "    'n_iter': 10\n",
    "}\n",
    "\n",
    "lr_results = cross_validate_with_tuning(\n",
    "    X, \n",
    "    y, \n",
    "    cat_cols, \n",
    "    num_cols, \n",
    "    lr_config, \n",
    "    k=3, \n",
    "    seed=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab6b358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Ridge Regression with hyperparameter tuning\n",
    "ridge_config = {\n",
    "    'model_class': Ridge,\n",
    "    'param_distributions': {\n",
    "        'alpha': loguniform(1e-3, 1e2),\n",
    "        'fit_intercept': [True, False],\n",
    "        'clean_outliers_flag': [True, False],\n",
    "        'standardize_cats_flag': [True, False],\n",
    "        'normalize_flag': [True, False],\n",
    "        'log_target': [True, False],\n",
    "        'feature_selection_k': [10, 20, 30, 40, 50, None]\n",
    "    },\n",
    "    'n_iter': 30\n",
    "}\n",
    "\n",
    "ridge_results = cross_validate_with_tuning(\n",
    "    X, \n",
    "    y, \n",
    "    cat_cols, \n",
    "    num_cols, \n",
    "    ridge_config, \n",
    "    k=3, \n",
    "    seed=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ebdfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Lasso Regression\n",
    "lasso_config = {\n",
    "    'model_class': Lasso,\n",
    "    'param_distributions': {\n",
    "        'alpha': loguniform(1e-3, 1e2),\n",
    "        'fit_intercept': [True, False],\n",
    "        'clean_outliers_flag': [True, False],\n",
    "        'standardize_cats_flag': [True, False],\n",
    "        'normalize_flag': [True, False],\n",
    "        'log_target': [True, False],\n",
    "        'feature_selection_k': [10, 20, 30, 40, 50, None],\n",
    "        'max_iter': 10000 # Increased from default 1000 to prevent convergence warnings\n",
    "    },\n",
    "    'n_iter': 30\n",
    "}\n",
    "\n",
    "lasso_results = cross_validate_with_tuning(\n",
    "    X, \n",
    "    y, \n",
    "    cat_cols, \n",
    "    num_cols, \n",
    "    lasso_config, \n",
    "    k=3, \n",
    "    seed=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8fb232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Random Forest\n",
    "rf_results = None\n",
    "\n",
    "if not DEMO_MODE:\n",
    "    rf_config = {\n",
    "        'model_class': RandomForestRegressor,\n",
    "        'param_distributions': {\n",
    "            'n_estimators': randint(100, 300),\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'min_samples_split': randint(2, 20),\n",
    "            'min_samples_leaf': randint(1, 10),\n",
    "            'max_features': ['sqrt', 0.5, 0.75, 1.0],\n",
    "            'n_jobs': [-1],\n",
    "            'random_state': SEED,\n",
    "            'clean_outliers_flag': [True, False],\n",
    "            'standardize_cats_flag': [True, False],\n",
    "            'normalize_flag': [True, False],\n",
    "            'log_target': [True, False],\n",
    "            'feature_selection_k': [10, 20, 30, 40, 50, None] # Try selecting top K features\n",
    "        },\n",
    "        'n_iter': 40\n",
    "    }\n",
    "\n",
    "    rf_results = cross_validate_with_tuning(\n",
    "        X, \n",
    "        y, \n",
    "        cat_cols, \n",
    "        num_cols, \n",
    "        rf_config, \n",
    "        k=3, \n",
    "        seed=SEED\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipping Random Forest in Demo Mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45536493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Hist Gradient Boosting Regressor\n",
    "hgb_results = None\n",
    "\n",
    "if not DEMO_MODE:\n",
    "    hgb_config = {\n",
    "        'model_class': HistGradientBoostingRegressor,\n",
    "        'param_distributions': {\n",
    "            'learning_rate': loguniform(0.01, 0.3),\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'max_leaf_nodes': randint(15, 63),\n",
    "            'l2_regularization': loguniform(1e-3, 10),\n",
    "            'max_iter': 1000,\n",
    "            'early_stopping': True,\n",
    "            'loss': ['absolute_error'],\n",
    "            'random_state': SEED,\n",
    "            'clean_outliers_flag': [True, False],\n",
    "            'standardize_cats_flag': [True, False],\n",
    "            'normalize_flag': [True, False],\n",
    "            'log_target': [True, False],\n",
    "            'feature_selection_k': [10, 20, 30, 40, 50, None]\n",
    "        },\n",
    "        'n_iter': 40\n",
    "    }\n",
    "\n",
    "    hgb_results = cross_validate_with_tuning(\n",
    "        X, \n",
    "        y, \n",
    "        cat_cols, \n",
    "        num_cols, \n",
    "        hgb_config, \n",
    "        k=3, \n",
    "        seed=SEED\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipping Hist Gradient Boosting in Demo Mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1256929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5: Gradient Boosting Regressor (Standard)\n",
    "gb_results = None\n",
    "\n",
    "if not DEMO_MODE:\n",
    "    gb_config = {\n",
    "        'model_class': GradientBoostingRegressor,\n",
    "        'param_distributions': {\n",
    "            'n_estimators': randint(50, 200),\n",
    "            'learning_rate': loguniform(0.01, 0.3),\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'min_samples_split': randint(2, 20),\n",
    "            'min_samples_leaf': randint(1, 10),\n",
    "            'subsample': [0.8, 0.9, 1.0],\n",
    "            'random_state': SEED,\n",
    "            'n_iter_no_change': [10],\n",
    "            'clean_outliers_flag': [True, False],\n",
    "            'standardize_cats_flag': [True, False],\n",
    "            'normalize_flag': [True, False],\n",
    "            'log_target': [True, False],\n",
    "            'feature_selection_k': [10, 20, 30, 40, 50, None]\n",
    "        },\n",
    "        'n_iter': 40\n",
    "    }\n",
    "\n",
    "    gb_results = cross_validate_with_tuning(\n",
    "        X, \n",
    "        y, \n",
    "        cat_cols, \n",
    "        num_cols, \n",
    "        gb_config, \n",
    "        k=3, \n",
    "        seed=SEED\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipping Gradient Boosting in Demo Mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4740b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 6: Extra Trees Regressor\n",
    "et_results = None\n",
    "\n",
    "if not DEMO_MODE:\n",
    "    et_config = {\n",
    "        'model_class': ExtraTreesRegressor,\n",
    "        'param_distributions': {\n",
    "            'n_estimators': randint(100, 300),\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'min_samples_split': randint(2, 20),\n",
    "            'min_samples_leaf': randint(1, 10),\n",
    "            'max_features': ['sqrt', 0.5, 0.75, 1],\n",
    "            'n_jobs': [-1],\n",
    "            'random_state': SEED,\n",
    "            'clean_outliers_flag': [True, False],\n",
    "            'standardize_cats_flag': [True, False],\n",
    "            'normalize_flag': [True, False],\n",
    "            'log_target': [True, False],\n",
    "            'feature_selection_k': [10, 20, 30, 40, 50, None]\n",
    "        },\n",
    "        'n_iter': 40\n",
    "    }\n",
    "\n",
    "    et_results = cross_validate_with_tuning(\n",
    "        X, \n",
    "        y, \n",
    "        cat_cols, \n",
    "        num_cols, \n",
    "        et_config, \n",
    "        k=3, \n",
    "        seed=SEED\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipping Extra Trees in Demo Mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830432df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 7: MLP Regressor (Neural Network)\n",
    "mlp_results = None\n",
    "\n",
    "if not DEMO_MODE:\n",
    "    mlp_config = {\n",
    "        'model_class': MLPRegressor,\n",
    "        'param_distributions': {\n",
    "            'hidden_layer_sizes': [(32,), (64,), (100,), (32, 16), (64, 32), (100, 50)],\n",
    "            'activation': ['relu', 'tanh'],\n",
    "            'solver': 'adam',\n",
    "            'alpha': loguniform(1e-5, 1e-1),\n",
    "            'learning_rate_init': loguniform(1e-4, 1e-2),\n",
    "            'max_iter': 2000,\n",
    "            'early_stopping': True,\n",
    "            'random_state': SEED,\n",
    "            'clean_outliers_flag': [True, False],\n",
    "            'standardize_cats_flag': [True, False],\n",
    "            'normalize_flag': [True, False],\n",
    "            'log_target': [True, False],\n",
    "            'feature_selection_k': [10, 20, 30, 40, 50, None]\n",
    "        },\n",
    "        'n_iter': 10\n",
    "    }\n",
    "\n",
    "    mlp_results = cross_validate_with_tuning(\n",
    "        X, \n",
    "        y, \n",
    "        cat_cols, \n",
    "        num_cols, \n",
    "        mlp_config, \n",
    "        k=3, \n",
    "        seed=SEED\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipping MLP in Demo Mode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d70f1b",
   "metadata": {},
   "source": [
    "<a id=\"final-model-selection\"></a>\n",
    "### Final Model Selection\n",
    "\n",
    "After evaluating all algorithms, we select the best performing model based on the **Mean Absolute Error (MAE)** from cross-validation.\n",
    "\n",
    "Once the best model class and its optimal hyperparameters (including preprocessing flags) are identified, we perform a **final refit**:\n",
    "1.  We take the **entire training dataset** (no train/val split).\n",
    "2.  We apply the optimal preprocessing pipeline (learning medians, encoders, etc., from the full data).\n",
    "3.  We train the model on this fully processed dataset.\n",
    "\n",
    "This ensures the model learns from the maximum amount of available data before making predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652a272f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models to find the best one\n",
    "results = {\n",
    "    'Linear Regression': lr_results,\n",
    "    'Ridge': ridge_results,\n",
    "    'Lasso': lasso_results,\n",
    "    'Random Forest': rf_results,\n",
    "    'Hist Gradient Boosting': hgb_results,\n",
    "    'Gradient Boosting': gb_results,\n",
    "    'Extra Trees': et_results,\n",
    "    'MLP': mlp_results,\n",
    "}\n",
    "\n",
    "# Filter out None results\n",
    "results = {k: v for k, v in results.items() if v is not None}\n",
    "\n",
    "# Select the best model based on the lowest mean CV score (MAE)\n",
    "best_model_name = min(results, key=lambda k: results[k]['mean_cv_score'])\n",
    "best_result = results[best_model_name]\n",
    "\n",
    "print(f\"Best Model Selected: {best_model_name}\")\n",
    "print(f\"Best CV MAE: £{best_result['mean_cv_score']:.2f} ± £{best_result['std_cv_score']:.2f}\")\n",
    "print(f\"Best Parameters: {best_result['best_params']}\\n\")\n",
    "\n",
    "# The function now returns the model already fitted on ALL available data\n",
    "# and the corresponding preprocessing artifacts\n",
    "best_model = best_result['best_estimator']\n",
    "final_artifacts = best_result['final_artifacts']\n",
    "\n",
    "print(f\"Final model ({best_model_name}) is ready and fitted on all data.\")\n",
    "\n",
    "# Visualize Feature Importance (if applicable)\n",
    "try:\n",
    "    # We need the processed feature names for the plot\n",
    "    # IMPORTANT: Must use the same preprocessing flags as the best model\n",
    "    preprocessing_params = final_artifacts.get('preprocessing_params', {})\n",
    "    \n",
    "    X_all_processed = preprocess_data(\n",
    "        X, \n",
    "        cat_cols, \n",
    "        num_cols, \n",
    "        artifacts=final_artifacts, \n",
    "        fit=False,\n",
    "        **preprocessing_params\n",
    "    )\n",
    "    \n",
    "    # Filter selected features if they were used during training\n",
    "    if 'selected_features' in final_artifacts:\n",
    "        X_all_processed = X_all_processed[final_artifacts['selected_features']]\n",
    "        \n",
    "    get_feature_importance(best_model, X_all_processed, model_class=type(best_model))\n",
    "except Exception as e:\n",
    "    print(f\"Could not plot feature importance: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb360862",
   "metadata": {},
   "source": [
    "<a id=\"deployment\"></a>\n",
    "## V. Deployment\n",
    "\n",
    "We use the final refitted model to generate predictions for the test dataset.\n",
    "\n",
    "**Process:**\n",
    "1.  **Load Test Data**: Read the `test.csv` file.\n",
    "2.  **Apply Preprocessing**: We use the **artifacts** (medians, outlier bounds, encoders, scaler) learned from the *full training set* to preprocess the test data. This ensures the test features are transformed exactly the same way as the training features.\n",
    "3.  **Predict**:\n",
    "    *   If the model was trained on log-transformed targets (`log_target=True`), we predict the log-price and then apply `np.expm1` to convert it back to the original currency scale.\n",
    "    *   Otherwise, we use the raw predictions.\n",
    "4.  **Save**: The results are saved to `data/test_predictions.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb33e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess test data\n",
    "test_df = pd.read_csv('data/test.csv').set_index('carID')\n",
    "\n",
    "# Use the artifacts from the final fit on all data\n",
    "test_processed = preprocess_test_data(test_df, final_artifacts)\n",
    "\n",
    "# Make predictions\n",
    "if final_artifacts.get('log_target', True):\n",
    "    test_predictions = np.expm1(best_model.predict(test_processed))\n",
    "else:\n",
    "    test_predictions = best_model.predict(test_processed)\n",
    "\n",
    "# Save predictions\n",
    "predictions_df = pd.DataFrame({'price': test_predictions}, index=test_df.index)\n",
    "predictions_df.to_csv('data/test_predictions.csv')\n",
    "\n",
    "print(f\"Predictions saved for {len(test_predictions)} test samples\")\n",
    "print(f\"Predicted price range: £{test_predictions.min():.2f} - £{test_predictions.max():.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
